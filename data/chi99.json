[
    {
        "title": "An empirical study of how people establish interaction: implications for CSCW session management models",
        "authors": "Steinar Kristoffersen, Fredrik Ljungberg",
        "abstract": "In this paper, we report the results of an empirical study of\nhow people, as part of their daily work activities, go about to\nestablish collaboration. We examine the empirical findings and\nrelate them to existing research on CSCW session management models,\ni.e., the mechanisms in CSCW systems that define the way in which\npeople can join together in collaboration. Existing models leave a\nlot to be desired, in particular because they tend to assume that\nindexical elements of interaction management are substitutable by\nobjective representation of artifacts. Based on the empirical\nfindings, we derive three principles to consider in the design of\nCSCW session management models."
    },
    {
        "title": "Chat circles",
        "authors": "Fernanda B. Viégas, Judith S. Donath",
        "abstract": "Although current online chat environments provide new\nopportunities for communication, they are quite constrained in\ntheir ability to convey many important pieces of social\ninformation, ranging from the number of participants in a\nconversation to the subtle nuances of expression that enrich face\nto face speech. In this paper we present Chat Circles, an abstract\ngraphical interface for synchronous conversa-tion. Here, presence\nand activity are made manifest by changes in color and form,\nproximity-based filtering intuitively breaks large groups into\nconversational clusters, and the archives of a conversation are\nmade visible through an integrated history interface. Our goal in\nthis work is to create a richer environment for online\ndiscussions."
    },
    {
        "title": "Social, individual and technological issues for groupware calendar systems",
        "authors": "Leysia Palen",
        "abstract": "Designing and deploying groupware is difficult. Groupware\nevaluation and design are often approached from a single\nperspective, with a technologically-, individually-, or\nsocially-centered focus. A study of Groupware Calendar Systems\n(GCSs) highlights the need for a synthesis of these multiple\nperspectives to fully understand the adoption challenges these\nsystems face. First, GCSs often replace existing calendar\nartifacts, which can impact users calendaring habits and in turn\ninfluence technology adoption decisions. Second, electronic\ncalendars have the potential to easily share contextualized\ninformation publicly over the computer network, creating\nopportunities for peer judgment about time allocation and raising\nconcerns about privacy regulation. However, this situation may also\nsupport coordination by allowing others to make useful inferences\nabout ones schedule. Third, the technology and the social\nenvironment are in a reciprocal, co-evolutionary relationship: the\nuse context is affected by the constraints and affordances of the\ntechnology, and the technology also co-adapts to the environment in\nimportant ways. Finally, GCSs, despite being below the horizon of\neveryday notice, can affect the nature of temporal coordination\nbeyond the expected meeting scheduling practice."
    },
    {
        "title": "The design and evaluation of a high-performance soft keyboard",
        "authors": "I. Scott MacKenzie, Shawn X. Zhang",
        "abstract": "The design and evaluation of a high performance soft keyboard\nfor mobile systems are described. Using a model to predict the\nupper-bound text entry rate for soft keyboards, we designed a\nkeyboard layout with a predicted upper-bound entry rate of 58.2\nwpm. This is about 35% faster than the predicted rate for a QWERTY\nlayout. We compared our design (OPTI) with a QWERTY layout in a\nlongitudinal evaluation using five participants and 20 45-minute\nsessions of text entry. Average entry rates for OPT1 increased from\n17.0 wpm initially to 44.3 wpm at session 20. The average rates\nexceeded those for the QWERTY layout after the 10 session (about 4\nhours of practice). A regression equation (R = .997) in the form of\nthe power-law of learning predicts that our upper-bound prediction\nwould be reach at about session 50."
    },
    {
        "title": "Non-keyboard QWERTY touch typing: a portable input interface for the mobile user",
        "authors": "Mikael Goldstein, Robert Book, Gunilla Alsiö, Silvia Tessa",
        "abstract": "Using traditional mobile input devices results in decreased\neffectiveness and efficiency. To improve usability issues a\nportable Non-Keyboard QWERTY touch-typing paradigm that supports\nthe mobile touch-typing user is presented and investigated. It\nrequires negligible training time. Pressure sensors strapped to the\nfingertips of gloves detect which finger is depressed. A language\nmodel based on lexical and syntactic knowledge transforms the\ndepressed finger stroke sequence into real words and sentences.\nDifferent mobile input QWERTY paradigms (miniaturised, floating and\nNon-Keyboard) have been compared with full-size QWERTY. Among the\nmobile input paradigms, the Non-Keyboard fared significantly\nbetter, both regarding character error rate and subjective\nratings."
    },
    {
        "title": "Implications for a gesture design tool",
        "authors": "Allan Christian Long, Jr., James A. Landay, Lawrence A. Rowe",
        "abstract": "Interest in pen-based user interfaces is growing rapidly. One\npotentially useful feature of pen-based user interfaces is\ngestures, that is, a mark or stroke that causes a command to\nexecute. Unfortunately, it is difficult to design gestures that are\neasy 1) for computers to recognize and 2) for humans to learn and\nremember. To investigate these problems, we built a prototype tool\ntypical fo those used for designing gesture sets. An experiment was\nthen performed to gain insight into the gesture design process and\nto evaluate this style of tool. The experiment confirmed that\ngesture design is very difficult and suggested several ways in\nwhich current tools can be improved. The most important improvement\nis to make the tools more active and provide more guidance for\ndesigners. This paper describes the gesture design tool, the\nexperiment, and its results."
    },
    {
        "title": "Object manipulation in virtual environments: relative size matters",
        "authors": "Yanqing Wang, Christine L. MacKenzie",
        "abstract": "An experiment was conducted to systematically investigate\ncombined effects of controller, cursor and target size on\nmultidimensional object manipulation in a virtual environment. It\nwas found that it was the relative size of controller, cursor and\ntarget that significantly affe&d object transportation and\norientation processes. There were significant interactions between\ncontroller size and cursor size as well as between cursor size and\ntarget size on the total task completion time, transportation time,\norientation time and spatial errors. The same size of controller\nand cursor improved object manipulation speed, and the same size of\ncursor and target generally facilitated object manipulation\naccuracy, regardless of their absolute sizes. Implications of these\nfindings for human-computer interaction design are discussed."
    },
    {
        "title": "Exploring bimanual camera control and object manipulation in 3D graphics interfaces",
        "authors": "Ravin Balakrishnan, Gordon Kurtenbach",
        "abstract": "We explore the use of the non-dominant hand to control a virtual\ncamera while the dominant hand performs other tasks in a virtual 3D\nscene. Two experiments and an informal study are presented which\nevaluate this interaction style by comparing it to the status-quo\nunimanual interaction. In the first experiment, we find that for a\ntarget selection task, performance using the bimanual technique was\n20% faster. Experiment 2 compared performance in a more complicated\nobject docking task. Performance advantages are shown, however,\nonly after practice. Free-form 3D painting was explored in the user\nstudy. In both experiments and in the user study participants\nstrongly preferred the bimanual technique. The results also\nindicate that user preferences concerning bimanual interaction may\nbe driven by factors other than simple time-motion performance\nadvantages."
    },
    {
        "title": "Towards usable VR: an empirical study of user interfaces for immersive virtual environments",
        "authors": "Robert W. Lindeman, John L. Sibert, James K. Hahn",
        "abstract": "This paper reports empirical results from a study into the use\nof 2D widgets in 3D immersive virtual environments. Several\nresearchers have proposed the use of 2D interaction techniques in\n3D environments, however little empirical work has been done to\ntest the usability of such approaches. We present the results of\ntwo experiments conducted on low-level 2D manipulation tasks within\nan immersive virtual environment. We empirically show that the\naddition of passive-haptic feedback for use in precise UI\nmanipulation tasks can significantly increase user performance.\nFurthermore, users prefer interfaces that provide a physical\nsurface, and that allow them to work with interface widgets in the\nsame visual field of view as the objects they are modifying."
    },
    {
        "title": "Socially translucent systems: social proxies, persistent conversation, and the design of “babble”",
        "authors": "Thomas Erickson, David N. Smith, Wendy A. Kellogg, Mark Laff, John T. Richards, Erin Bradner",
        "abstract": "We take as our premise that it is possible and desirable to\ndesign systems that support social processes. We describe Loops, a\nproject which takes this approach to supporting computer-mediated\ncommunication (CMC) through structural and intemctive properties\nsuch as persistence and a minimalist graphical representation of\nusers and their activities that we call a social proxy. We discuss\na prototype called Babble that has been used by our group for over\na year, and has been deployed to six other groups at the Watson\nlabs for about two months. We describe usage experiences, lessons\nlearned, and next steps."
    },
    {
        "title": "The elements of computer credibility",
        "authors": "B. J. Fogg, Hsiang Tseng",
        "abstract": "Given the importance of credibility in computing products, the\nresearch on computer credibility is relatively small. To enhance\nknowledge about computers and credibility, we define key terms\nrelating to computer credibility, synthesize the literature in this\ndomain, and propose three new conceptual frameworks for better\nunderstanding the elements of computer credibility. To promote\nfurther research, we then offer two perspectives on what computer\nusers evaluate when assessing credibility. We conclude by\npresenting a set of credibility-related terms that can serve in\nfuture research and evaluation endeavors."
    },
    {
        "title": "A better mythology for system design",
        "authors": "Jed Harris, Austin Henderson",
        "abstract": "The past decades have seen huge improvements in computer systems\nbut these have proved difficult to translate into comparable\nimprovements in the usability and social integration of computers.\nWe believe that the problem is a deeply rooted set of assumptions\nabout how computer systems should be designed, and about who should\nbe doing that design. Human organizations are continually evolving to meet changing\ncircumstances of resource and need. In contrast, computers are\nquite rigid, incapable of adaptation on their own. Therefore when\ncomputer systems are incorporated into human organizations, those\norganizations must adapt the computers to changing circumstances.\nThis adaptation is another human activity that technology should\nsupport, but our design philosophies are oddly silent about it. This paper explores the origins of these problems in the norms\ndeveloped for managing human organizations, proposes partial\nsolutions that can be implemented with current systems technology,\nand speculates about the long-term potential for radical\nimprovements in system design."
    },
    {
        "title": "Nomadic radio: scaleable and contextual notification for wearable audio messaging",
        "authors": "Nitin Sawhney, Chris Schmandt",
        "abstract": "Mobile workers need seamless access to communication and\ninformation services on portable devices. However current solutions\noverwhelm users with intrusive and ambiguous notifications. In this\npaper, we describe scaleable auditory techniques and a contextual\nnotification model for providing timely information, while\nminimizing interruptions. Users actions influence local adaptation\nin the model. These techniques are demonstrated in Nomadic Radio,\nan audio-only wearable computing platform."
    },
    {
        "title": "Tangible progress: less is more in Somewire audio spaces",
        "authors": "Andrew Singer, Debby Hindus, Lisa Stifelman, Sean White",
        "abstract": "We developed four widely different interfaces for users of\nSomewire, a prototype audio-only media space. We informally studied\nusers experiences with the two screen- based interfaces. We\nprototyped a non-screen-based interface as an example of a novel\ntangible interface for a communication system. We explored the\nconflict between privacy and simplicity of representation, and\nidentified two unresolved topics: the role of audio quality and the\nprospects for scsiling audio spaces beyond a single Workgroup.\nFinally, we formulated a set of design guidelines for control and\nrepresentation in audio spaces, as follows: GUIs are not\nwell-suited to audio spaces, users do not require control over\nlocalization or other audio attributes, and awareness of other\nusers presence is desirable."
    },
    {
        "title": "Whisper: a wristwatch style wearable handset",
        "authors": "Masaaki Fukumoto, Yoshinobu Tonomura",
        "abstract": "Whisper is a new wrist-worn handset, which is used by inserting\nthe fingertip into the ear canal. A received signal is conveyed\nfrom a wrist-mounted actuator to the ear canal via the hand and a\nfinger by bone conduction. The users voice is captured by a\nmicrophone mounted on the inside of the wrist. All components of\nWhisper can be mounted on the wrist, and usability does not de-\ncrease if the size of components is miniaturized. So, both\nwearability and usability can be achieved together. The way Whisper\nis operated is similar to that of an ordinary telephone handset.\nThus, onlookers may not look upon Whispers operation as talking to\noneself, even if the associated PDA is controlled by voice\ncommands. Whis- per is especially effective in a noisy environment.\nSignals received via bone conduction can be heard clearly in the\npresence of noise without raising the volume (-12 dB at noise = 90\ndB(A) in comparison to cellular phone hand- set). Whisper is also\neffective in avoiding the annoying problem of the users voice being\nraised in a noisy situa- tion. Feedback of the users utterance is\nboosted by bone conduction when covering the ear canal with a\nfingertip, then the users voice does not need to raised in the\npres- ence of noise (-6 dB at noise = 90 dB(A) in comparison to\ncellular phone handset). Whisper is useful as a voice interface for\na wrist-worn PDA and cellular phone."
    },
    {
        "title": "i-LAND: an interactive landscape for creativity and innovation",
        "authors": "Norbert A. Streitz, Jörg Geißler, Torsten Holmer, Shin'ichi Konomi, Christian Müller-Tomfelde, Wolfgang Reischl, Petra Rexroth, Peter Seitz, Ralf Steinmetz",
        "abstract": "We describe the i-LAND environment which constitutes an example\nof our vision of the workspaces of the future, in this case\nsupporting cooperative work of dynamic teams with changing needs.\ni-LAND requires and provides new forms of human-computer\ninteraction and new forms of computer-supported cooperative work.\nIts design is based on an integration of information and\narchitectural spaces, implications of new work practices and an\nempirical requirements study informing our design. i-LAND consists\nof several roomware components, i.e. computer-aug- mented objects\nintegrating room elements with information technology. We present\nthe current realization of i-LAND in terms of an interactive\nelectronic wall, an interactive table, two computer-enhanced\nchairs, and two bridges for the Passage-mechanism. This is\ncomplemented by the description of the creativity support\napplication and the technological infrastructure. The paper is\naccompanied by a video figure in the CHI99 video program."
    },
    {
        "title": "Logjam: a tangible multi-person interface for video logging",
        "authors": "Jonathan Cohen, Meg Withgott, Philippe Piernot",
        "abstract": "This paper describes the evolution, implementation, and use of\nlogjam, a system for video logging. The system features a\ngame-board that senses the location and identities of pieces placed\nupon it. The board is the interface that enables a group of people\nto log video footage together. We report on some of the surprising\nphysical and social dynamics that we have observed in multi-person\nlogging sessions using the system."
    },
    {
        "title": "Time-compression: systems concerns, usage, and benefits",
        "authors": "Nosa Omoigui, Liwei He, Anoop Gupta, Jonathan Grudin, Elizabeth Sanocki",
        "abstract": "With the proliferation of online multimedia content and the\npopularity of multimedia streaming systems, it is increasingly\nuseful to be able to skim and browse multimedia quickly. A key\ntechnique that enables quick browsing of multimedia is\ntime-compression. Prior research has described how speech can be\ntime-compressed (shortened in duration) while preserving the pitch\nof the audio. However, client-server systems providing this\nfunctionality have not been available. In this paper, we first describe the key tradeoffs faced by\ndesigners of streaming multimedia systems deploying\ntime-compression. The implementation tradeoffs primarily impact the\ngranularity of time-compression supported (discrete vs. continuous)\nand the latency (wait-time) experienced by users after adjusting\ndegree of time-compression. We report results of user studies\nshowing impact of these factors on the average- compression-rate\nachieved. We also present data on the usage patterns and benefits\nof time compression. Overall, we show significant time-savings for\nusers and that considerable flexibility is available to the\ndesigners of client-server streaming systems with time\ncompression."
    },
    {
        "title": "SWEETPEA: software tools for programmable embodied agents",
        "authors": "Michael Kaminsky, Paul Dourish, W. Keith Edwards, Anthony LaMarca, Michael Salisbury, Ian Smith",
        "abstract": "Programmable Embodied Agents are portable, wireless, interactive\ndevices embodying specific, differentiable, interactive\ncharacteristics. They take the form of identifiable characters who\nreside in the physical world and interact directly with users. They\ncan act as an out-of-band communication channel between users, as\nproxies for system components or other users, or in a variety of\nother roles. Traditionally, research into such devices has been\nbased on costly custom hardware. In this paper, we report on our\nexplorations of the space of physical character-based interfaces\nbuilt on recently available stock consumer hardware platforms,\nstructured around an initial framework of applications."
    },
    {
        "title": "Sympathetic interfaces: using a plush toy to direct synthetic characters",
        "authors": "Michael Patrick Johnson, Andrew Wilson, Bruce Blumberg, Christopher Kline, Aaron Bobick",
        "abstract": "We introduce the concept of a sympathetic inter$ace for\ncontrolling an animated synthetic character in a 3D virtual\nenvironment. A plush doll embedded with wireless sensors is used to\nmanipulate the virtual character in an iconic and intentional\nmanner. The interface extends from the novel physical input device\nthrough interpretation of sensor data to the behavioral brain of\nthe virtual character. We discuss the design of the interface and\nfocus on its latest instantiation in the Swamped! exhibit at\nSIGGRAPH 98. We also present what we learned from hundreds of\ncasual users, who ranged from young children to adults."
    },
    {
        "title": "Principles of mixed-initiative user interfaces",
        "authors": "Eric Horvitz",
        "abstract": "Recent debate has centered on the relative promise of focusing\nuser-interface research on developing new metaphors and tools that\nenhance users abilities to directly manipulate objects versus\ndirecting effort toward developing interface agents that provide\nautomation. In this paper, we review principles that show promise\nfor allowing engineers to enhance human-computer interaction\nthrough an elegant coupling of automated services with direct\nmanipulation. Key ideas will be highlighted in terms of the Lookout\nsystem for scheduling and meeting management."
    },
    {
        "title": "An exploration into supporting artwork orientation in the user interface",
        "authors": "George W. Fitzmaurice, Ravin Balakrishnan, Gordon Kurtenbach, Bill Buxton",
        "abstract": "Rotating a piece of paper while drawing is an integral and\nalmost subconscious part of drawing with pencil and paper. In a\nsimilar manner, the advent of lightweight pen-based computers allow\ndigital artwork to be rotated while drawing by rotating the entire\ncomputer. Given this type of manipulation we explore the\nimplications for the user interface to support artwork orientation.\nFirst we describe an exploratory study to further motivate our work\nand characterize how artwork is manipulated while drawing. After\npresenting some possible UI approaches to support artwork\norientation, we define a new solution called a rotating user\ninterface (RUIs). We then discuss design issues and requirements\nfor RUIs based on our exploratory study."
    },
    {
        "title": "An alternative way of drawing",
        "authors": "Roope Raisamo",
        "abstract": "Current object-oriented drawing programs have an established way\nof drawing in which the shape of an object is controlled by\nmanipulating control points. While the control points are intuitive\nin their basic use, it is not clear whether they make more complex\ndrawing tasks manageable for the average user. In this paper we\ndescribe an alternative way of drawing and editing a drawing using\nnew direct manipulation tools. Our approach resembles sculpting in\ntwo dimensions: the user begins with a large block and uses\ndifferent tools to give it the desired shape. We also present a\nuser evaluation in which the users could try our new tools and\ncompare them to their previous experience of control points, The\nusers claimed to understand the operations better with our tools\nthan if they had needed to use curves and control points. However,\nour tools were better suited for sketching the artwork than for\nmaking very detailed drawings."
    },
    {
        "title": "The strategic use of CAD: an empirically inspired, theory-based course",
        "authors": "Suresh K. Bhavnani, Bonnie E. John, Ulrich Flemming",
        "abstract": "The inefficient use of complex computer systems has been widely\nreported. These studies show the persistence of inefficient methods\ndespite many years of experience and formal training. To counteract\nthis phenomenon, we present the design of a new course, called the\nStrategic Use of CAD. The course aims at teaching students\nefficient strategies to use a computer-aided drafting system\nthrough a two-pronged approach. Learning to See teaches students to\nrecognize opportunities to use efficient strategies by studying the\nnature of the task, and Learning to Do teaches students to\nimplement the strategies. Results from a pilot experiment show that\nthis approach had a positive effect on the strategic behavior of\nstudents who did not exhibit knowledge of efficient strategies\nbefore the class, and had no effect on the strategic behavior of\nthose who did. Strategic training can thus assist users in\nrecognizing opportunities to use efficient strategies. We present\nthe ramifications of these results on the design of training and\nfuture experiments."
    },
    {
        "title": "Implementing interface attachments based on surface representations",
        "authors": "Dan R. Olsen, Jr., Scott E. Hudson, Thom Verratti, Jeremy M. Heiner, Matt Phelps",
        "abstract": "This paper describes an architecture for supporting interface\nattuchments - small interactive programs which are designed to\naugment the functionality of other applications. This architecture\nis designed to work with a diverse set of conventional\napplications, but require only a minimal set of hooks into those\napplications. In order to achieve this, the work described here\nconcentrates on what we will call observational attachments, a\nsubclass of attachments that operate primarily by observing and\nmanipulating the surface representations of applications - that is\nthe visual information that applications would normally display on\nthe screen or print. These attachments can be thought of as looking\nover the shoulder of the user to assist with various tasks. By\nrequiring very little modification to, or help from, the\napplications they augment, this approach supports the creation of a\nset of uniform services that can be applied across a more diverse\nset of applications than traditional approaches."
    },
    {
        "title": "A visual medium for programmatic control of interactive applications",
        "authors": "Luke S. Zettlemoyer, Robert St. Amant",
        "abstract": "The VisMap system provides for visual manipulation of arbitrary\noff-the-shelf applications, through an applications graphical user\ninterface. VisMaps API-independent control has advantages for tasks\nthat can benefit from direct access to the functions of the user\ninterface. We describe the design goals and architecture of the\nsystem, and we discuss two applications, a user-controlled visual\nscripting program and an autonomous solitaire-playing program,\nwhich together demonstrate some of the capabilities and limitations\nof the approach."
    },
    {
        "title": "Should we leverage natural-language knowledge? An analysis of user errors in a natural-language-style programming language",
        "authors": "Amy Bruckman, Elizabeth Edwards",
        "abstract": "Should programming languages use natural-language-like syntax?\nUnder what circumstances? What sorts of errors do novice\nprogrammers make? Does using a natural- language-like programming\nlanguage lead to user errors? In this study, we read the entire\nonline interactions of sixteen children who issued a total of\n35,047 commands on MOOSE Crossing, an educational MUD for children,\nWe counted and categorized the errors made. A total d 2,970 errors\nwere observed. We define natural-language errors as those errors in\nwhich the user failed to distinguish between English and code,\nissuing an incorrect command that was more English-like than the\ncorrect one. A total of 314 natural-language errors were observed.\nIn most of those errors, the child was able to correct the problem\neither easily (41.1% of the time) or with some effort (20.7%).\nNatural-language errors were divided into five categories. In order\nfrom most to least frequent, they are: syntax errors, guessing a\ncommand name by supplying an arbitrary English word, literal\ninterpretation of metaphor, assuming the system is keeping more\nstate information than is actually the case, and errors of operator\nprecedence and combination. We believe that these error rates are\nwithin acceptable limits, and conclude that leveraging users\nnatural-language knowledge is for many applications an effective\nstrategy for designing end-user-programming languages."
    },
    {
        "title": "Testing pointing device performance and user assessment with the ISO 9241, Part 9 standard",
        "authors": "Sarah A. Douglas, Arthur E. Kirkpatrick, I. Scott MacKenzie",
        "abstract": "The IS0 9241, Part 9 Draft International Standard for testing\ncomputer pointing devices proposes an evaluation of performance and\ncomfort. In this paper we evaluate the scientific validity and\npracticality of these dimensions for two pointing devices for\nlaptop computers, a finger-controlled isometric joystick and a\ntouchpad. Using a between-subjects design, evaluation of\nperformance using the measure of throughput was done for\none-direction and multi-directional pointing and selecting. Results\nshow a significant difference in throughput for the\nmulti-directional task, with the joystick 27% higher; results for\nthe one-direction task were non-significant. After the experiment,\nparticipants rated the device for comfort, including operation,\nfatigue, and usability. The questionnaire showed no overall\ndifference in the responses, and a significant statistical\ndifference in only the question concerning force required to\noperate the device - the joystick requiring slightly more force.\nThe paper concludes with a discussion of problems in implementing\nthe IS0 standard and recommendations for improvement."
    },
    {
        "title": "Touch-sensing input devices",
        "authors": "Ken Hinckley, Mike Sinclair",
        "abstract": "We can touch things, and our senses tell us when our hands are\ntouching something. But most computer input devices cannot detect\nwhen the user touches or releases the device or some portion of the\ndevice. Thus, adding touch sensors to input devices offers many\npossibilities for novel interaction techniques. We demonstrate the\nTouchTrackball and the Scrolling TouchMouse, which use unobtrusive\ncapacitance sensors to detect contact from the users hand without\nrequiring pressure or mechanical actuation of a switch. We further\ndemonstrate how the capabilities of these devices can be matched to\nan implicit interaction technique, the On-Demand Interface, which\nuses the passive information captured by touch sensors to fade in\nor fade out portions of a display depending on what the user is\ndoing; a second technique uses explicit, intentional interaction\nwith touch sensors for enhanced scrolling. We present our new\ndevices in the context of a simple tax- onomy of tactile input\ntechnologies. Finally, we discuss the properties of touch-sensing\nas an input channel in general."
    },
    {
        "title": "The Hotbox: efficient access to a large number of menu-items",
        "authors": "Gordon Kurtenbach, George W. Fitzmaurice, Russell N. Owen, Thomas Baudel",
        "abstract": "The proliferation of multiple toolbars and UI widgets\naround the perimeter of application windows is an indication that\nthe traditional GUI design of a single menubar is not\nsufficient to support large scale applications with numerous\nfunctions. In this paper we describe a new widget which is an\nenhancement of the traditional menubar which dramatically\nincreases menu-item capacity. This widget, called the Hotbox\ncombines several GUI techniques which are generally used\nindependently: accelerator keys, modal dialogs, pop-up/pull down\nmenus, radial menus, marking menus and menubars. These techniques\nare fitted together to create a single, easy to learn yet fast to\noperate GUI widget which can handle significantly more\nmenu-items than the traditional GUI menubar. We\ndescribe the design rationale of the Hotbox and its effectiveness\nin a large scale commercial application. While the Hotbox was\ndeveloped for a particular application domain, the widget itself\nand the design rationale are potentially useful in other\ndomains."
    },
    {
        "title": "Combining observations of intentional and unintentional behaviors for human-computer interaction",
        "authors": "Yoshinori Kuno, Tomoyuki Ishiyama, Satoru Nakanishi, Yoshiaki Shirai",
        "abstract": "Human interfaces are usually designed to respond only to\nintentional human behaviors. However, humans show unintentional\nbehaviors as well. They can convey useful information to realize\nuser-friendly human interfaces. This paper presents how to combine\nobservations of both types of behaviors by taking two human-\nmachine systems: a gesture-based interface and an intelligent\nwheelchair. In the first system, intentional hand gestures are\nchosen using unintentional behaviors. In the second system, near\nunintentional behaviors following intentional behaviors can be used\nto control the wheelchair motion. Experimental systems working in\nreal time have been developed. Operational experiments prove our\napproach promising."
    },
    {
        "title": "Manual and gaze input cascaded (MAGIC) pointing",
        "authors": "Shumin Zhai, Carlos Morimoto, Steven Ihde",
        "abstract": "This work explores a new direction in utilizing eye gaze for\ncomputer input. Gaze tracking has long been considered as an\nalternative or potentially superior pointing method for computer\ninput. We believe that many fundamental limitations exist with\ntraditional gaze pointing. In particular, it is unnatural to\noverload a perceptual channel such as vision with a motor control\ntask. We therefore propose an alternative approach, dubbed MAGIC\n(Manual And Gaze Input Cascaded) pointing. With such an approach,\npointing appears to the user to be a manual task, used for fine\nmanipulation and selection. However, a large portion of the cursor\nmovement is eliminated by warping the cursor to the eye gaze area,\nwhich encompasses the target. Two specific MAGIC pointing\ntechniques, one conservative and one liberal, were designed,\nanalyzed, and implemented with an eye tracker we developed. They\nwere then tested in a pilot study. This early- stage exploration\nshowed that the MAGIC pointing techniques might offer many\nadvantages, including reduced physical effort and fatigue as\ncompared to traditional manual pointing, greater accuracy and\nnaturalness than traditional gaze pointing, and possibly faster\nspeed than manual pointing. The pros and cons of the two techniques\nare discussed in light of both performance data and subjective\nreports."
    },
    {
        "title": "Inferring intent in eye-based interfaces: tracing eye movements with process models",
        "authors": "Dario D. Salvucci",
        "abstract": "While current eye-based interfaces offer enormous potential for\nefficient human-computer interaction, they also manifest the\ndifficulty of inferring intent from user eye movements. This paper\ndescribes how fixation tracing facilitates the interpretation of\neye movements and improves the flexibility and usability of\neye-based interfaces. Fixation tracing uses hidden Markov models to\nmap user actions to the sequential predictions of a cognitive\nprocess model. In a study of eye typing, results show that fixation\ntracing generates significantly more accurate interpretations than\nsimpler methods and allows for more flexibility in designing usable\ninterfaces. Implications for future research in eye-based\ninterfaces and multimodal interfaces are discussed."
    },
    {
        "title": "Direct combination",
        "authors": "Simon Holland, Daniel Oppenheim",
        "abstract": "This paper reports on Direct Combination, a new user interaction\ntechnique. Direct Combination may be viewed variously as: a\nsystematic extension to Direct Manipulation; a concise navigational\nframework to help users find the operations they need; and as a\nframework to make a greater range and variety of operations\navailable to the user, without overburdening user or interface\ndesigner. While Direct Combination may be seen as an extension of\nDirect Manipulation, it may also be applied to a wide range of user\ninteraction styles, including even command line interfaces.\nExamples from various hypothetical systems and from an implemented\nsystem are presented. This paper argues that Direct Combination is\napplicable not just to problem seeking or design oriented domains\n(where the technique originated) but is generally applicable. A\nvariety of new interaction styles for Direct Combination are\npresented. The generalisation of Direct Combination to the\nn-dimensional case is presented."
    },
    {
        "title": "Footprints: history-rich tools for information foraging",
        "authors": "Alan Wexelblat, Pattie Maes",
        "abstract": "Inspired by Hill and Hollans original work [7], we have been\ndeveloping a theory of interaction history and building tools to\napply this theory to navigation in a complex information space. We\nhave built a series of tools - map, paths, annota- tions and\nsignposts - based on a physical-world navigation metaphor. These\ntools have been in use for over a year. Our user study involved a\ncontrolled browse task and showed that users were able to get the\nsame amount of work done with significantly less effort."
    },
    {
        "title": "Design guidelines for landmarks to support navigation in virtual environments",
        "authors": "Norman G. Vinson",
        "abstract": "Unfamiliar, large-scale virtual environments are difficult to\nnavigate. This paper presents design guidelines to ease navigation\nin such virtual environments. The guidelines presented here focus\non the design and placement of landmarks in virtual environments.\nMoreover, the guidelines are based primarily on the extensive\nempirical literature on navigation in the real world. A rationale\nfor this approach is provided by the similarities between\nnavigational behavior in real and virtual environments."
    },
    {
        "title": "Single display groupware: a model for co-present collaboration",
        "authors": "Jason Stewart, Benjamin B. Bederson, Allison Druin",
        "abstract": "We introduce a model for supporting collaborative work between\npeople that are physically close to each other. We call this model\nSingle Display Groupware (SDG). In this paper, we describe the\nmodel, comparing it to more traditional remote collaboration, We\ndescribe the requirements that SDG places on computer technology,\nand our understanding of the benefits and costs of SDG systems.\nFinally, we describe a prototype SDG system that we built and the\nresults of a usability test we ran with 60 elementary school\nchildren."
    },
    {
        "title": "The GAZE groupware system: mediating joint attention in multiparty communication and collaboration",
        "authors": "Roel Vertegaal",
        "abstract": "In this paper, we discuss why, in designing multiparty mediated\nsystems, we should focus first on providing non-verbal cues which\nare less redundantly coded in speech than those normally conveyed\nby video. We show how conveying one such cue, gaze direction, may\nsolve two problems in multiparty mediated communication and\ncollaboration: knowing who is talking to whom, and who is talking\nabout what. As a candidate solution, we present the GAZE Groupware\nSystem, which combines support for gaze awareness in multiparty\nmediated communication and collaboration with small and linear\nbandwidth requirements. The system uses an advanced, desk- mounted\neyetracker to metaphorically convey gaze awareness in a 3D virtual\nmeeting room and within shared documents."
    },
    {
        "title": "Video helps remote work: speakers who need to negotiate common ground benefit from seeing each other",
        "authors": "Elizabeth S. Veinott, Judith Olson, Gary M. Olson, Xiaolan Fu",
        "abstract": "More and more organizations are forming teams that are not\nco-located. These teams communicate via email, fax, telephone and\naudio conferences, and sometimes video. The question often arises\nwhether the cost of video is worth it. Previous research has shown\nthat video makes people more satisfied with the work, but it doesnt\nhelp the quality of the work itself. There is one exception;\nnegotiation tasks are measurably better with video. In this study,\nwe show that the same effect holds for a more subtle form of\nnegotiation, when people have to negotiate meaning in a\nconversation. We compared the performance and communication of\npeople explaining a map route to each other. Half the pairs have\nvideo and audio connections, half only audio. Half of the pairs\nwere native speakers of English; the other half were non-native\nspeakers, those presumably who have to negotiate meaning more. The\nresults showed that non-native speaker pairs did benefit from the\nvideo; native speakers did not. Detailed analysis of the\nconversational strategies showed that with video, the non-native\nspeaker pairs spent proportionately more effort negotiating common\nground."
    },
    {
        "title": "Designing multimedia for learning: narrative guidance and narrative construction",
        "authors": "Lydia Plowman, Rosemary Luckin, Diana Laurillard, Matthew Stratfold, Josie Taylor",
        "abstract": "Narrative is fundamental to the ways we make sense of texts of\nall kinds because it provides structure and coherence, but it is\ndifficult to see how this works in the context of multimedia\ninteractive learning environments (MILES). We tested our hypotheses\nabout the form and function of narrative in MILES by developing\nthree versions of material on CD-ROM which had different narrative\nstructures and analysed the impact of the different versions on\nlearner behaviour. We present a theoretical framework in which we\nexplain the concepts of narrative guidance and narrative\nconstruction and their application to the design of MILES."
    },
    {
        "title": "Interactive 3D sound hyperstories for blind children",
        "authors": "Maruricio Lumbreras, Jaime Sánchez",
        "abstract": "Interactive software is currently used for learning and\nentertainment purposes. This type of software is not very common\namong blind children because most computer games and electronic\ntoys do not have appropriate interfaces to be accessible without\nvisual cues. This study introduces the idea of interactive hyperstories\ncarried out in a 3D acoustic virtual world for blind children. We\nhave conceptualized a model to design hyperstories. Through\nAudioDoom we have an application that enables testing cognitive\ntasks with blind children. The main research question underlying\nthis work explores how audio- based entertainment and spatial sound\nnavigable experiences can create cognitive spatial structures in\nthe minds of blind children. AudioDoom presents first person experiences through exploration\nof interactive virtual worlds by using only 3D aural\nrepresentations of the space."
    },
    {
        "title": "Designing PETS: a personal electronic teller of stories",
        "authors": "Allison Druin, Jamie Montemayor, Jim Hendler, Britt McAlister, Angela Boltman, Eric Fiterman, Aurelie Plaisant, Alex Kruskal, Hanne Olsen, Isabella Revett, Thomas Plaisant Schwenn, Lauren Sumida, Rebecca Wagner",
        "abstract": "We have begun the development of a new robotic pet that can\nsupport children in the storytelling process. Children can build\ntheir own pet by snapping together the modular animal parts of the\nPETS robot. After their pet is built, children can tell\nstories using the My Pets software. These stories can\nthen be acted out by their robotic pet. This video paper describes\nthe motivation for this research and the design process of our\nintergenerational design team in building the first PETS\nprototypes. We will discuss our progress to date and our focus for\nthe future."
    },
    {
        "title": "Visual profiles: a critical component of universal access",
        "authors": "Julie A. Jacko, Max A. Dixon, Robert H. Rosa, Jr., Ingrid U. Scott, Charles J. Pappas",
        "abstract": "This research focuses on characterizing visually impaired\ncomputer users performance on graphical user interfaces by linking\nclinical assessments of low vision with visual icon identification.\nThis was accomplished by evaluating user performance on basic\nidentification and selection tasks within a graphical user\ninterface, comparing partially sighted user performance with fully\nsighted user performance, and linking task performance to specific\nprofiles of visual impairment. Results indicate that visual acuity,\ncontrast sensitivity, visual field and color perception were\nsignificant predictors of task performance. In addition, icon size\nand background color significantly influenced performance.\nSuggestions for future research are provided. Keywords"
    },
    {
        "title": "NotePals: lightweight note sharing by the group, for the group",
        "authors": "Richard C. Davis, James A. Landay, Victor Chen, Jonathan Huang, Rebecca B. Lee, Frances C. Li, James Lin, Charles B. Morrey, III, Ben Schleimer, Morgan N. Price, Bill N. Schilit",
        "abstract": "NotePals is a lightweight note sharing system that gives group\nmembers easy access to each others experiences through their\npersonal notes. The system allows notes taken by group members in\nany context to be uploaded to a shared repository. Group members\nview these notes with browsers that allow them to retrieve all\nnotes taken in a given context or to access notes from other\nrelated notes or documents. This is possible because NotePals\nrecords the context in which each note is created (e.g., its\nauthor, subject, and creation time). The system is lightweight\nbecause it fits easily into group members regular note- taking\npractices, and uses informal, ink-based user interfaces that run on\nportable, inexpensive hardware. In this paper we describe NotePals,\nshow how we have used it to share our notes, and present our\nevaluations of the system."
    },
    {
        "title": "Flatland: new dimensions in office whiteboards",
        "authors": "Elizabeth D. Mynatt, Takeo Igarashi, W. Keith Edwards, Anthony LaMarca",
        "abstract": "Flatland is an augmented whiteboard interface designed for\ninformal office work. Our research investigates approaches to\nbuilding an augmented whiteboard in the context of continuous, long\nterm office use. In particular, we pursued three avenues of\nresearch based on input from user studies: techniques for the\nmanagement of space on the board, the ability to flexibly apply\nbehaviors to support varied application semantics, and mechanisms\nfor managing history on the board. Unlike some previously reported\nwhiteboard systems, our design choices have been influenced by a\ndesire to support long-term, informal use in an individual office\nsetting."
    },
    {
        "title": "Palette: a paper interface for giving presentations",
        "authors": "Les Nelson, Satoshi Ichimura, Elin Rønby Pedersen, Lia Adams",
        "abstract": "The Palette is a digital appliance designed for intuitive\ncontrol of electronic slide shows. Current interfaces demand too\nmuch of our attention to permit effective computer use in\nsituations where we can not give the technology our fullest\nconcentration. The Palette uses index cards that are printed with\nslide content that is easily identified by both humans and\ncomputers. The presenter controls the presentation by directly\nmanipulating the cards. The Palette design is based on our\nobservation of presentations given in a real work setting. Our\nexperiences using the system are described, including new practices\n(e.g., collaborative presentation, enhanced notetaking) that arise\nfrom the affordances of this new approach. This system is an\nexample of a new interaction paradigm called tacit\ninteraction that supports users who can spare very little\nattention to a computer interface."
    },
    {
        "title": "TouchCounters: designing interactive electronic labels for physical containers",
        "authors": "Paul Yarin, Hiroshi Ishii",
        "abstract": "We present TouchCounters, an integrated system of electronic\nmodules, physical storage containers, and shelving surfaces for the\nsupport of collaborative physical work. Through physical sensors\nand local displays, TouchCounters record and display usage history\ninformation upon physical storage containers, thus allowing access\nto this information during the performance of real-world tasks. A\ndistributed communications network allows this data to be exchanged\nwith a server, such that users can access this information from\nremote locations as well. Based upon prior work in ubiquitous computing and tangible\ninterfaces, TouchCounters incorporate new techniques, including\nusage history tracking for physical objects and multi-display\nvisualization. This paper describes the components, interactions,\nimplementation, and conceptual approach of the TouchCounters\nsystem."
    },
    {
        "title": "Bridging physical and virtual worlds with electronic tags",
        "authors": "Roy Want, Kenneth P. Fishkin, Anuj Gujar, Beverly L. Harrison",
        "abstract": "The role of computers in the modern office has divided our\nactivities between virtual interactions in the realm of the\ncomputer and physical interactions with real objects within the\ntraditional office infrastructure. This paper extends previous work\nthat has attempted to bridge this gap, to connect physical objects\nwith virtual representations or computational functionality, via\nvarious types of tags. We discuss a variety of scenarios we have\nimplemented using a novel combination of inexpensive, unobtrusive\nand easy to use RFID tags, tag readers, portable computers and\nwireless networking. This novel combination demonstrates the\nutility of invisibly, seamlessly and portably linking physical\nobjects to networked electronic services and actions that are\nnaturally associated with their form."
    },
    {
        "title": "Augmented surfaces: a spatially continuous work space for hybrid computing environments",
        "authors": "Jun Rekimoto, Masanori Saitoh",
        "abstract": "This paper describes our design and implementation of a computer\naugmented environment that allows users to smoothly interchange\ndigital information among their portable computers, table and wall\ndisplays, and other physical objects. Supported by a camera-based\nobject recognition system, users can easily integrate their\nportable computers with the pre-installed ones in the environment.\nUsers can use displays projected on tables and walls as a spatially\ncontinuous extension of their portable computers. Using an\ninteraction technique called hyperdragging, users can transfer\ninformation from one computer to another, by only knowing the\nphysical relationship between them. We also provide a mechanism for\nattaching digital data to physical objects, such as a videotape or\na document folder, to link physical and digital spaces."
    },
    {
        "title": "Urp: a luminous-tangible workbench for urban planning and design",
        "authors": "John Underkoffler, Hiroshi Ishii",
        "abstract": "We introduce a system for urban planning - called Urp -that\nintegrates functions addressing a broad range of the fields\nconcerns into a single, physically based workbench setting. The I/O\nBulb infrastructure on which the application is based allows\nphysical architectural models placed on an ordinary table surface\nto cast shadows accurate for arbitrary times of day; to throw\nreflections off glass facade surfaces; to affect a real-time and\nvisually coincident simulation of pedestrian-level windflow; and so\non. We then use comparisons among Urp and several\nearlier I/O Bulb applications as the basis for an\nunderstanding of luminous-tangible interactions, which result\nwhenever an interface distributes meaning and functionality between\nphysical objects and visual information projectively coupled to\nthose objects. Finally, we briefly discuss two issues common to all\nsuch systems, offering them as informal thought-tools for the\ndesign and analysis of luminous-tangible interfaces."
    },
    {
        "title": "PingPongPlus: design of an athletic-tangible interface for computer-supported cooperative play",
        "authors": "Hiroshi Ishii, Craig Wisneski, Julian Orbanes, Ben Chun, Joe Paradiso",
        "abstract": "This paper introduces a novel interface for digitally-augmented\ncooperative play. We present the concept of the athletic-tangible\ninterface, a new class of interaction which uses tangible objects\nand full-body motion in physical spaces with digital augmentation.\nWe detail the implementation of PingPongPlus, a reactive ping-pong\ntable, which features a novel sound-based ball tracking technology.\nThe game is augmented and transformed with dynamic graphics and\nsound, determined by the position of impact, and the rhythm and\nstyle of play. A variety of different modes of play and initial\nexperiences with PingPongPlus are also described."
    },
    {
        "title": "Eye tracking the visual search of click-down menus",
        "authors": "Michael D. Byrne, John R. Anderson, Scott Douglass, Michael Matessa",
        "abstract": "Click-down (or pull-down) menus have long been a key component\nof graphical user interfaces, yet we know surprisingly little about\nhow users actually interact with such menus. Nilsens [8] study on\nmenu selection has led to the development of a number of models of\nhow users perform the task [6, 21. However, the validity of these\nmodels has not been empirically assessed with respect to eye\nmovements (though [l] presents some interesting data that bear on\nthese models). The present study is an attempt to provide data that\ncan help refine our understanding of how users interact with such\nmenus."
    },
    {
        "title": "Cognitive modeling demonstrates how people use anticipated location knowledge of menu items",
        "authors": "Anthony J. Hornof, David E. Kieras",
        "abstract": "This research presents cognitive models of a person selecting an\nitem from a familiar, ordered, pull-down menu. Two different models\nprovide a good fit with human data and thus two different possible\nexplanations for the low- level cognitive processes involved in the\ntask. Both models assert that people make an initial eye and hand\nmovement to an anticipated target location without waiting for the\nmenu to appear. The first model asserts that a person knows the\nexact location of the target item before the menu appears, but the\nmodel uses nonstandard Fitts law coefficients to predict mouse\npointing time. The second model asserts that a person would only\nknow the approximate location of the target item, and the model\nuses Fitts law coefficients better supported by the literature.\nThis research demonstrates that people can develop considerable\nknowledge of locations in a visual task environment, and that more\nwork regarding Fitts law is needed."
    },
    {
        "title": "Learning and performing by exploration: label quality measured by latent semantic analysis",
        "authors": "Rodolfo Soto",
        "abstract": "Models of learning and performing by exploration assume that the\nsemantic similarity between task descriptions and labels on display\nobjects (e.g., menus, tool bars) controls in part the users search\nstrategies. Nevertheless, none of the models has an objective way\nto compute semantic similarity. In this study, Latent Semantic\nAnalysis (LSA) was used to compute semantic similarity between task\ndescriptions and labels in an applications menu system.\nParticipants performed twelve tasks by exploration and they were\ntested for recall after a l-week delay. When the labels in the menu\nsystem were semantically similar to the task descriptions, subjects\nperformed the tasks faster. LSA could be incorporated into any of\nthe current models, and it could be used to automate the evaluation\nof computer applications for ease of learning and performing by\nexploration."
    },
    {
        "title": "MOBILE: user-centered interface building",
        "authors": "Angel R. Puerta, Eric Cheng, Tunhow Ou, Justin Min",
        "abstract": "Interfhce builders are popular tools for designing and\ndeveloping graphical user interfaces. These tools, however, are\nengineering-centered; they operate mainly on windows and widgets. A\ntypical interface builder does not offer any specific support for\nuser-centered interface design, a methodology recognized as\ncritical for effective user interface design. We present MOBILE\n(Model-Based Inter&xx Layout Editor) an interface building tool\nthat iidly supports user-centered design and that guides the\nhtafhce building process by using user-task models and a knowledge\nbase of interface design guidelines. The approach in MOBILE has the\nimportant added benefit of being useful in both top-down and\nbottom-up interface design strategies."
    },
    {
        "title": "The context toolkit: aiding the development of context-enabled applications",
        "authors": "Daniel Salber, Anind K. Dey, Gregory D. Abowd",
        "abstract": "Context-enabled applications are just emerging and promise\nricher interaction by taking environmental context into account.\nHowever, they are difficult to build due to their distributed\nnature and the use of unconventional sensors. The concepts of\ntoolkits and widget libraries in graphical user interfaces has been\ntremendously successtil, allowing programmers to leverage off\nexisting building blocks to build interactive systems more easily.\nWe introduce the concept of context widgets that mediate between\nthe environment and the application in the same way graphical\nwidgets mediate between the user and the application. We illustrate\nthe concept of context widgets with the beginnings of a widget\nlibrary we have developed for sensing presence, identity and\nactivity of people and things. We assess the success of our\napproach with two example context-enabled applications we have\nbuilt and an existing application to which we have added\ncontext-sensing capabilities."
    },
    {
        "title": "Getting more out of programming-by-demonstration",
        "authors": "Richard G. McDaniel, Brad A. Myers",
        "abstract": "Programming-by-demonstration (PBD) can be used to create tools\nand methods that eliminate the need to learn difficult computer\nlanguages. Gamut is a PBD tool that nonprogrammers can use to\ncreate a broader range of interactive software, including games,\nsimulations, and educational software, than they can with other PBD\ntools. To do this, Gamut provides advanced interaction techniques\nthat make it easier for a developer to express all aspects of an\napplication. These techniques include a simplified way to\ndemonstrate new examples, called nudges, and a way to highlight\nobjects to show they are important. Also, Gamut includes new\nobjects and metaphors like the deck-of-cards metaphor for\ndemonstrating collections of objects and randomness, guide objects\nfor demonstrating relationships that the system would find too\ndifficult to guess, and temporal ghosts which simplify showing\nrelationships with the recent past. These techniques were tested in\na formal setting with nonprogrammers to evaluate their\neffectiveness."
    },
    {
        "title": "Navigation as multiscale pointing: extending Fitts' model to very high precision tasks",
        "authors": "Yves Guiard, Michel Beaudouin-Lafon, Denis Mottet",
        "abstract": "Fitts pointing model has proven extremely useful for\nunderstanding basic selection in WIMP user interfaces. Yet todays\ninterfaces involve more complex navigation within electronic\nenvironments. As navigation amounts to a form of multi-scale\npointing, Fitts model can be applied to these more complex tasks.\nWe report the results of a preliminary pointing experiment that\nshows that users can handle higher levels of task difficulty with\ntwo-scale rather than traditional one-scale pointing control. Also,\nin tasks with very high-precision hand movements, performance is\nhigher with a stylus than with a mouse."
    },
    {
        "title": "Authoring animated Web pages using “contact points”",
        "authors": "Pete Faraday, Alistair Sutcliffe",
        "abstract": "This paper explores how contact points or co-references between\nan animation and text should be designed in web pages. Guidelines\nare derived from an eye tracking study. A dynamic HTML authoring\ntool is described which supports these requirements. An evaluation\nstudy is reported in which four designs of animation in web pages\nwere tested."
    },
    {
        "title": "Performance evaluation of input devices in trajectory-based tasks: an application of the steering law",
        "authors": "Johnny Accot, Shumin Zhai",
        "abstract": "Choosing input devices for interactive systems that best suit\nusers needs remains a challenge, especially consid- ering the\nincreasing number of devices available. The choice often has to be\nmade through empirical evalua- tions. The most frequently used\nevaluation task hitherto is target acquisition, a task that can be\naccurately modeled by Fitts law. However, todays use of computer\ninput devices has gone beyond target acquisition alone. In\nparticular, we often need to perform trajectory-based tasks, such\nas drawing, writing, and navigation. This paper illustrates how a\nrecently discovered model, the steering law, can be applied as an\nevaluation paradigm complementary to Fitts law. We tested five\ncommonly used computer input devices in two steering tasks, one\nlinear and one circular. Results showed that subjects performance\nwith the five devices could be generally classified into three\ngroups in the following order: 1. the tablet and the mouse, 2. the\ntrackpoint, 3. the touch- pad and the trackball. The steering law\nproved to hold for all five devices with greater than 0.98\ncorrelation. The ability to generalize the experimental results and\nthe limitations of the steering law are also discussed."
    },
    {
        "title": "Symphony: a case study in extending learner-centered design through process space analysis",
        "authors": "Chris Quintana, Jim Eng, Andrew Carra, Hsin-Kai Wu, Elliot Soloway",
        "abstract": "We are exploring a new class of tools for learners: scaffolded\nintegrated tool environments (or SITES), which address the needs of\nlearners trying to engage in new, complex work processes. A crucial\nphase within a learner-centered design approach for SITE design\ninvolves analyzing the work process to identify areas where\nlearners need support to engage in the process. Here we discuss the\ndesign of Symphony, a SITE for high-school science students.\nSpecifically, we discuss how the process-space model helped us\nanalyze the science inquiry process to help us identify a detailed\nset of learner needs, leading to a full set of process scaffolding\nstrategies for Symphony."
    },
    {
        "title": "The reader's helper: a personalized document reading environment",
        "authors": "Jamey Graham",
        "abstract": "Over the last two centuries, reading styles have shifted away\nfrom the reading of documents from beginning to end and toward the\nskimming of documents in search of relevant information. This trend\ncontinues today where readers, often confronted with an\ninsurmountable amount of text, seek more efficient methods of\nextracting relevant information from documents. In this paper, a\nnew document reading environment is introduced called the Readers\nHelperTM, which supports the reading of electronic and paper\ndocuments. The Readers Helper analyzes documents and produces a\nrelevance score for each of the readers topics of interest, thereby\nhelping the reader decide whether the document is actually worth\nskimming or reading. Moreover, during the analysis process, topic\nof interest phrases are automatically annotated to help the reader\nquickly locate relevant information. A new information\nvisualization tool, called the ThumbarTM, is used in conjunction\nwith relevancy scoring and automatic annotation to portray a\ncontinuous, dynamic thumb-nail representation of the document. This\nfurther supports rapid navigation of the text."
    },
    {
        "title": "VR's frames of reference: a visualization technique for mastering abstract multidimensional information",
        "authors": "Marilyn C. Salzman, Chris Dede, R. Bowen Loftin",
        "abstract": "This paper describes a research study that investigated how\ndesigners can use frames of reference (egocentric, exocentric, and\na combination of the two) to support the mastery of abstract\nmultidimensional information. The primary focus of this study was\nthe relationship between FORs and mastery; the secondary focus was\non other factors (individual characteristics and interaction\nexperience) that were likely to influence the relationship between\nFORs and mastery. This studys outcomes (1) clarify how FORs work in\nconjunction with other factors in shaping mastery, (2) highlight\nstrengths and weaknesses of different FORs, (3) demonstrate the\nbenefits of providing multiple FORs, and (4) provide the basis for\nour recommendations to HCI researchers and designers."
    },
    {
        "title": "FotoFile: a consumer multimedia organization and retrieval system",
        "authors": "Allan Kuchinsky, Celine Pering, Michael L. Creech, Dennis Freeze, Bill Serra, Jacek Gwizdka",
        "abstract": "FotoFile is an experimental system for multimedia\norganization and retrieval, based upon the design goal of making\nmultimedia content accessible to non-expert users. Search and\nretrieval are done in terms that are natural to the task. The\nsystem blends human and automatic annotation methods. It extends\ntextual search, browsing, and retrieval technologies to support\nmultimedia data types."
    },
    {
        "title": "Hyper Mochi Sheet: a predictive focusing interface for navigating and editing nested networks through a multi-focus distortion-oriented view",
        "authors": "Masashi Toyoda, Etsuya Shibayama",
        "abstract": "Multi-focus distortion-oriented views are useful in viewing\nlarge information on a small screen, but still have problems in\nmanaging multiple foci during editing. The user may have to\nnavigate information space by focusing and defocusing multiple\nparts to obtain multi-focus layouts that change according to\nvarious editing situations. As a result, it becomes haphazard to\nnavigate and edit large nested networks such as hypertexts. We\npropose a user interface for quickly obtaining desirable layouts.\nThe interface uses two techniques: focus size prediction and\npredictive focus selection. These techniques are based on a user\ntest and experiences in applications. We also describe two example\napplications."
    },
    {
        "title": "Excentric labeling: dynamic neighborhood labeling for data visualization",
        "authors": "Jean-Daniel Fekete, Catherine Plaisant",
        "abstract": "The widespread use of information visualization is hampered by\nthe lack of effective labeling techniques. An informal taxonomy of\nlabeling methods is proposed. We then describe excentric labeling,\na new dynamic technique to label a neighborhood of objects located\naround the cursor. This technique does not intrude into the\nexisting interaction, it is not computationally intensive, and was\neasily applied to several visualization applications. A pilot study\nwith eight subjects indicates a strong speed benefit over a zoom\ninterface for tasks that involve the exploration of large numbers\nof objects. Observations and comments from users are presented."
    },
    {
        "title": "Embodiment in conversational interfaces: Rea",
        "authors": "J. Cassell, T. Bickmore, M. Billinghurst, L. Campbell, K. Chang, H. Vilhjálmsson, H. Yan",
        "abstract": "In this paper, we argue for embodied corrversational characters\nas the logical extension of the metaphor of human - computer\ninteraction as a conversation. We argue that the only way to fully\nmodel the richness of human I&+ to-face communication is to\nrely on conversational analysis that describes sets of\nconversational behaviors as fi~lfilling conversational functions,\nboth interactional and propositional. We demonstrate how to\nimplement this approach in Rea, an embodied conversational agent\nthat is capable of both multimodal input understanding and output\ngeneration in a limited application domain. Rea supports both\nsocial and task-oriented dialogue. We discuss issues that need to\nbe addressed in creating embodied conversational agents, and\ndescribe the architecture of the Rea interface."
    },
    {
        "title": "Emotional interfaces for interactive aardvarks: designing affect into social interfaces for children",
        "authors": "Erik Strommen, Kristin Alexander",
        "abstract": "Character-based social interfaces present a unique opportunity\nto integrate emotion into technology interactions. The present\npaper reports on the use of three emotional interactions (humor,\npraise, and affection) in the audio interfaces for two\ncharacter-based interactive learning toys. The reasons for\nselecting the emotions used, the design rationale for their\napplication, and findings from usability testing are reviewed. It\nis suggested that as a form of pretend play-acting akin to\npuppetry, social interfaces can engage the emotions of users in a\nvariety of beneficial ways."
    },
    {
        "title": "Bridging strategies for VR-based learning",
        "authors": "Tom Moher, Andrew Johnson, Stellan Ohlsson, Mark Gillingham",
        "abstract": "A distributed immersive virtual environment was deployed as a\ncomponent of a pedagogical strategy for teaching third grade\nchildren that the Earth is round. The displacement strategy is\nbased on the theory that fundamental conceptual change requires an\nalternative cognitive starting point which doesnt invoke the\nfeatures of pre-existing models. While the VR apparatus helped to\nestablish that alternative framework, conceptual change was\nstrongly influenced by the bridging activities which related that\nexperience to the target domain. Simple declarations of relevance\nproved ineffective. A more articulated bridging process involving\nphysical models was effective for some children, but the multiple\nrepresentations employed required too much model-matching for\nothers."
    },
    {
        "title": "The tangled Web we wove: a taskonomy of WWW use",
        "authors": "Michael D. Byrne, Bonnie E. John, Neil S. Wehrle, David C. Crow",
        "abstract": "A prerequisite to the effective design of user interfaces is an\nunderstanding of the tasks for which that interface will actually\nbe used. Surprisingly little task analysis has appeared for one of\nthe most discussed and fastest-growing computer applications,\nbrowsing the World-Wide Web (WWW). Based on naturally-collected\nverbal protocol data, we present a taxonomy of tasks undertaken on\nthe WWW. The data reveal that several previous claims about\nbrowsing behavior are questionable, and suggests that that\nwidget-centered approaches to interface design and evaluation may\nbe incomplete with respect to good user interfaces for the Web."
    },
    {
        "title": "An empirical evaluation of user interfaces for topic management of Web sites",
        "authors": "Brian Amento, Will Hill, Loren Terveen, Deborah Hix, Peter Ju",
        "abstract": "Topic management is the task of gathering, evaluating,\norganizing, and sharing a set of web sites for a specific topic.\nCurrent web tools do not provide adequate support for this task. We\ncreated the TopicShop system to address this need. TopicShop\nincludes (1) a webcrawler that discovers relevant web sites and\nbuilds site profiles, and (2) user interfaces for exploring and\norganizing sites. We conducted an empirical study comparing user\nperformance with TopicShop vs. YahooTM. TopicShop subjects found\nover 80% more high-quality sites (where quality was determined by\nindependent expert judgements) while browsing only 8 1% as many\nsites and completing their task in 89% of the time. The site\nprofile data that TopicShop provides - in particular, the number of\npages on a site and the number of other sites that link to it - was\nthe key to these results, as users exploited it to identify the\nmost promising sites quickly and easily."
    },
    {
        "title": "Visualizing implicit queries for information management and retrieval",
        "authors": "Mary Czerwinski, Susan Dumais, George Robertson, Susan Dziadosz, Scott Tiernan, Maarten van Dantzich",
        "abstract": "In this paper, we describe the use of similarity metrics in a\nnovel visual environment for storing and retrieving favorite web\npages. The similarity metrics, called Implicit Queries, are\nused to automatically highlight stored web pages that are related\nto the currently selected web page. Two experiments explored how\nusers manage their personal web information space with and without\nthe Implicit Query highlighting and later retrieve their stored web\npages. When storing and organizing web pages, users with Implicit\nQuery highlighting generated slightly more categories. Implicit\nQueries also led to faster web page retrieval time, although the\nresults were not statistically significant."
    },
    {
        "title": "Patterns of entry and correction in large vocabulary continuous speech recognition systems",
        "authors": "Clare-Marie Karat, Christine Halverson, Daniel Horn, John Karat",
        "abstract": "A study was conducted to evaluate user performance and\nsatisfaction in completion of a set of text creation tasks using\nthree commercially available continuous speech recognition systems.\nThe study also compared user performance on similar tasks using\nkeyboard input. One part of the study (Initial Use) involved 24\nusers who enrolled, received training and carried out practice\ntasks, and then completed a set of transcription and composition\ntasks in a single session. In a parallel effort (Extended Use),\nfour researchers used speech recognition to carry out real work\ntasks over 10 sessions with each of the three speech recognition\nsoftware products. This paper presents results from the Initial Use\nphase of the study along with some preliminary results from the\nExtended Use phase. We present details of the kinds of usability\nand system design problems likely in current systems and several\ncommon patterns of error correction that we found."
    },
    {
        "title": "Mutual disambiguation of recognition errors in a multimodel architecture",
        "authors": "Sharon Oviatt",
        "abstract": "As a new generation of multimodal/media systems begins to define\nitself, researchers are attempting to learn how to combine\ndifferent modes into strategically integrated whole systems. In\ntheory, well designed multimodal systems should be able to\nintegrate complementary modalities in a manner that supports mutual\ndisambiguation (MD) of errors and leads to more robust performance.\nIn this study, over 2,000 multimodal utterances by both native and\naccented speakers of English were processed by a multimodal system,\nand then logged and analyzed. The results confirmed that multimodal\nsystems can indeed support significant levels of MD, and also\nhigher levels of MD for the more challenging accented users. As a\nresult, although speech recognition as a stand-alone performed far\nmore poorly for accented speakers, their multimodal recognition\nrates did not differ from those of native speakers. Implications\nare discussed for the development of future multimodal\narchitectures that can perform in a more robust and stable manner\nthan individual recognition technologies. Also discussed is the\ndesign of interfaces that support diversity in tangible ways, and\nthat function well under challenging real-world usage\nconditions,"
    },
    {
        "title": "Model-based and empirical evaluation of multimodal interactive error correction",
        "authors": "Bernhard Suhm, Brad Myers, Alex Waibel",
        "abstract": "Our research addresses the problem of error correction in speech\nuser interfaces. Previous work hypothesized that switching modality\ncould speed up interactive correction of recognition errors\n(so-called multimodal error correction). We present a user study\nthat compares, on a dictation task, multimodal error correction\nwith conventional interactive correction, such as speaking again,\nchoosing Tom a list, and keyboard input. Results show that\nmultimodal correction is faster than conventional correction\nwithout keyboard input, but slower than correction by typing for\nusers with good typing skills. Furthermore, while users initially\nprefer speech, they learn to avoid ineffective correction\nmodalities with experience. To extrapolate results from this user\nstudy we developed a performance model of multimodal interaction\nthat predicts input speed including time needed for error\ncorrection. We apply the model to estimate the impact of\nrecognition technology improvements on correction speeds and the\ninfluence of recognition accuracy and correction method on the\nproductivity of dictation systems. Our model is a first step\ntowards formalizing multimodal (recognition-based) interaction."
    },
    {
        "title": "Cooperative inquiry: developing new technologies for children with children",
        "authors": "Allison Druin",
        "abstract": "In todays homes and schools, children are emerging as frequent\nand experienced users of technology [3, 14]. As this trend\ncontinues, it becomes increasingly important to ask if we are\nfulfilling the technology needs of our children. To answer this\nquestion, I have developed a research approach that enables young\nchildren to have a voice throughout the technology development\nprocess. In this paper, the techniques of cooperative\ninquiry will be described along with a theoretical framework\nthat situates this work in the HCI literature. Two examples of\ntechnology resulting from this approach will be presented, along\nwith a brief discussion on the design-centered learning of\nteam researchers using cooperative inquiry."
    },
    {
        "title": "Projected realities: conceptual design for cultural effect",
        "authors": "William Gaver, Anthony Dunne",
        "abstract": "As a part of a European Union sponsored project, we have\nproposed a system which aggregates peoples expressions over a\nwidening network of public electronic displays in a massive Dutch\nhousing development. Reflecting ideas from contemporary arts as\nwell as from research on media spaces, this is an example of a\nconceptual design intended to produce meaningful effects on a local\nculture. In this paper, we describe the methods and ideas that led\nto this proposal, as an example of research on technologies from\nthe traditions of artist-designers."
    },
    {
        "title": "Customer-focused design data in a large, multi-site organization",
        "authors": "Paula Curtis, Tammy Heiserman, David Jobusch, Mark Notess, Jayson Webb",
        "abstract": "Qualitative user-centered design processes such as contextual\ninquiry can generate huge amounts of data to be organized,\nanalyzed, and represented. When you add the goal of spreading the\nresultant understanding to the far reaches of a large, multi-site\norganization, many practical barriers emerge. In this paper we describe experience creating and communicating\nrepresentations of contextually derived user data in a large,\nmulti-site product development organization. We describe how we\ninvolved a distributed team in data collection and analysis and how\nwe made the data representations portable. We then describe how we\nhave engaged over 200 people from five sites in thinking through\nthe user data and its implications on product design."
    }
]