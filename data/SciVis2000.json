[
  {
    "abstract": "We present a new technique for extracting regions of interest (ROI) applying a local watershed transformation. The proposed strategy for computing catchment basins in a given region of interest is based on a rain-falling simulation. Unlike the standard watershed algorithms, which flood the complete (gradient magnitude of an) image, the proposed approach allows us to perform this task locally. Thus, a controlled region growth is performed, saving time and reducing the memory requirement especially when applied on volume data. A second problem arising from the standard watershed transformation is the over-segmented result and the lack of sound criteria for merging the computed basins. For overcoming this drawback, we present a basin-merging strategy introducing four criteria for merging adjacent basins. The threshold values applied in this strategy are derived from the user input and match rather the attributes of the selected object than of all objects in the image. In doing so, the user is not required to adjust abstract numbers, but to simply select a coarse region of interest. Moreover, the proposed algorithm is not limited to the 2D case. As we show in this work, it is suitable for volume data processing as well. Finally, we present the results of applying the proposed approach on several example images and volume data sets.", 
    "authors": "Stoev, S.L.;Strasser, W.", 
    "title": "Extracting regions of interest applying a local watershed transformation"
  }, 
  {
    "abstract": "We present a new visibility determination algorithm for interactive virtual endoscopy. The algorithm uses a modified version of template-based ray casting to extract a view dependent set of potentially visible voxels from volume data. The voxels are triangulated by Marching Cubes and the triangles are rendered onto the display by a graphics accelerator. Early ray termination and space leaping are used to accelerate the ray casting step and a quadtree subdivision algorithm is used to reduce the number of cast rays. Compared to other recently proposed rendering algorithms for virtual endoscopy, our rendering algorithm does not require a long preprocessing step or a high-end graphics workstation, but achieves interactive frame rates on a standard PC equipped with a low-cost graphics accelerator.", 
    "authors": "Hietala, R.;Oikarinen, J.", 
    "title": "A visibility determination algorithm for interactive virtual endoscopy"
  }, 
  {
    "abstract": "We propose a novel approach for segmentation and digital cleansing of endoscopic organs. Our method can be used for a variety of segmentation needs with little or no modification. It aims at fulfilling the dual and often conflicting requirements of a fast and accurate segmentation and also eliminates the undesirable partial volume effect which contemporary approaches cannot. For segmentation and digital cleansing, we use the peculiar characteristics exhibited by the intersection of any two distinct-intensity regions. To detect these intersections we cast rays through the volume, which we call the segmentation rays as they assist in the segmentation. We then associate a certain task of reconstruction and classification with each intersection the ray detects. We further use volumetric contrast enhancement to reconstruct surface lost by segmentation (if any), which aids in improving the quality of the volume rendering.", 
    "authors": "Lakare, S.;Wan, M.;Sato, M.;Kaufman, A.", 
    "title": "3D digital cleansing using segmentation rays"
  }, 
  {
    "abstract": "We present CEASAR, a centerline extraction algorithm that delivers smooth, accurate and robust results. Centerlines are needed for accurate measurements of length along winding tubular structures. Centerlines are also required in automatic virtual navigation through human organs, such as the colon or the aorta, as they are used to control movement and orientation of the virtual camera. We introduce a concise but general definition of a centerline, and provide an algorithm that finds the centerline accurately and rapidly. Our algorithm is provably correct for general geometries. Our solution is fully automatic, which frees the user from having to engage in data preprocessing. For a number of test datasets, we show the smooth and accurate centerlines computed by our CEASAR algorithm on a single 194 MHz MIPS R10000 CPU within five minutes.", 
    "authors": "Bitter, I.;Sato, M.;Bender, M.;McDonnell, K.T.;Kaufman, A.;Wan, M.", 
    "title": "CEASAR: a smooth, accurate and robust centerline extraction algorithm"
  }, 
  {
    "abstract": "Richly expressive information visualizations are difficult to design and rarely found. Few software tools can generate multidimensional visualizations at all, let alone incorporate artistic detail. Althrough, it is a great efficiency to reuse these visualizations with new data, the associated artistic detail is rarely reusable. The Relational Visualization Notation is a new technique and toolkit for specifying highly expressive graphical representations of data without traditional programming. We seek to discover the accessible power of this notation\u2014both its graphical expressiveness and its ease of re-use. Towards this end we have used the system to reconstruct Minard\u2019s visualization of Napoleon\u2019s Russian campaign of 1812. The resulting image is strikingly similar to the original, and the design is straightforward to construct. Furthermore, the design permitted by the notation can be directly reused to visualize Hitler\u2019s WWII defeat before Moscow. This experience leads us to believe that artistically expressive visualizations can be made to be reusable.", 
    "authors": "Humphrey, Matthew C. ", 
    "title": "Creating reusable visualizations with the relational visualization notation"
  }, 
  {
    "abstract": "We present a new hierarchical clustering and visualization algorithm called H-BLOB, which groups and visualizes cluster hierarchies at multiple levels-of-detail. Our method is fundamentally different to conventional clustering algorithms, such as C-means, K-means, or linkage methods that are primarily designed to partition a collection of objects into subsets sharing similar attributes. These approaches usually lack an efficient level-of-detail strategy that breaks down the visual complexity of very large datasets for visualization. In contrast, our method combines grouping and visualization in a two stage process constructing a hierarchical setting. In the first stage a cluster tree is computed making use of an edge contraction operator. Exploiting the inherent hierarchical structure of this tree, a second stage visualizes the clusters by computing a hierarchy of implicit surfaces. We believe that H-BLOB is especially suited for the visualization of very large datasets and for visual decision making in information visualization. The versatility of the algorithm is demonstrated using examples from visual data mining.", 
    "authors": "Sprenger, T.C.;Brunella, R.;Gross, M.H.", 
    "title": "H-BLOB: a hierarchical visual clustering method using implicit surfaces"
  }, 
  {
    "abstract": "As the size and complexity of data sets continues to increase, the development of user interfaces and interaction techniques that expedite the process of exploring that data must receive new attention. Regardless of the speed of rendering, it is important to coherently organize the visual process of exploration: this information both grants insights about the data to a user and can be used by collaborators to understand the results. To fulfil these needs, we present a spreadsheet-like interface to data exploration. The interface displays a 2-dimensional window into visualization parameter space which users manipulate as they search for desired results. Through tabular organization and a clear correspondence between parameters and results, the interface eases the discovery, comparison and analysis of the underlying data. Users can utilize operators and the integrated interpreter to further explore and automate the visualization process; using a method introduced in this paper, these operations can be applied to cells in different stacks of the interface. Via illustrations using a variety of data sets, we demonstrate the efficacy of this novel interface.", 
    "authors": "Jankun-Kelly, T.J.;Kwan-Liu Ma", 
    "title": "A spreadsheet interface for visualization exploration"
  }, 
  {
    "abstract": "In many applications of scientific visualization, a large quantity of data is being processed and displayed in order to enable a viewer to make informed and effective decisions. Since little data is perfect, there is almost always some degree of associated uncertainty. This uncertainty is an important part of the data and should be taken into consideration when interpreting the data. Uncertainty, however, should not overshadow the data values. Many methods that address the problem of visualizing data with uncertainty can distort the data and emphasize areas with uncertain values. We have developed a method for showing the uncertainty information together with data with minimal distraction. This method uses procedurally generated annotations which are deformed according to the uncertainty information. As another possible technique we propose distorting glyphs according to the uncertainty information.", 
    "authors": "Cedilnik, A.;Rheingans, P.", 
    "title": "Procedural annotation of uncertain information"
  }, 
  {
    "abstract": "The techniques for reducing the size of a volume dataset by preserving both the geometrical/topological shape and the information encoded in an attached scalar field are attracting growing interest. Given the framework of incremental 3D mesh simplification based on edge collapse, we propose an approach for the integrated evaluation of the error introduced by both the modification of the domain and the approximation of the field of the original volume dataset. We present and compare various techniques to evaluate the approximation error or to produce a sound prediction. A flexible simplification tool has been implemented, which provides a different degree of accuracy and computational efficiency for the selection of the edge to be collapsed. Techniques for preventing a geometric or topological degeneration of the mesh are also presented.", 
    "authors": "Cignoni, P.;Costanza, D.;Montani, C.;Rocchini, C.;Scopigno, R.", 
    "title": "Simplification of tetrahedral meshes with accurate error evaluation"
  }, 
  {
    "abstract": "We present a new method for the modeling of freehand collected three-dimensional ultrasound data. The model is piecewise linear and based upon progressive tetrahedral domains created by a subdivision scheme which splits a tetrahedron on on its longest edge and guarantees a valid tetrahedrization. Least squares error is used to characterize the model and an effective iterative technique is used to compute the values of the model at the vertices of the tetrahedral grid. Since the subdivision strategy is adaptive, the complexity of the model conforms to the complexity of the data leading to an extremely efficient and highly compressed volume model. The model is evaluated in real time using piecewise linear interpolation, and gives a medical professional the chance to see images which would not be possible using conventional ultrasound techniques.", 
    "authors": "Roxborough, T.;Nielson, G.M.", 
    "title": "Tetrahedron based, least squares, progressive volume models with application to freehand ultrasound data"
  }, 
  {
    "abstract": "Very large irregular-grid data sets are represented as tetrahedral meshes and may incur significant disk I/O access overhead in the rendering process. An effective way to alleviate the disk I/O overhead associated with rendering a large tetrahedral mesh is to reduce the I/O bandwidth requirement through compression. Existing tetrahedral mesh compression algorithms focus only on compression efficiency and cannot be readily integrated into the mesh rendering process, and thus demand that a compressed tetrahedral mesh be decompressed before it can be rendered into a 2D image. This paper presents an integrated tetrahedral mesh compression and rendering algorithm called Gatun, which allows compressed tetrahedral meshes to be rendered incrementally as they are being decompressed, thus leading to an efficient irregular grid rendering pipeline. Both compression and rendering algorithms in Gatun exploit the same local connectivity information among adjacent tetrahedra, and thus can be tightly integrated into a unified implementation framework. Our tetrahedral compression algorithm is specifically designed to facilitate the integration with an irregular grid renderer without any compromise in compression efficiency. A unique performance advantage of Gatun is its ability to reduce the runtime memory footprint requirement by releasing memory allocated to tetrahedra as early as possible.", 
    "authors": "Chuan-Kai Yang;Mitra, T.;Tzi-cker Chiueh", 
    "title": "On-the-fly rendering of losslessly compressed irregular volume data"
  }, 
  {
    "abstract": "We present two beneficial rendering extensions to the projected tetrahedra (PT) algorithm proposed by Shirley and Tuchman (1990). These extensions are compatible with any cell sorting technique, for example the BSP-XMPVO sorting algorithm for unstructured meshes. Using 3D texture mapping our first extension solves the longstanding problem of hardware-accelerated but accurate rendering of tetrahedral volume cells with arbitrary transfer functions. By employing 2D texture mapping our second extension realizes the hardware-accelerated rendering of multiple shaded isosurfaces within the PT algorithm without reconstructing the isosurfaces. Additionally, two methods are presented to combine projected tetrahedral volumes with isosurfaces. The time complexity of all our algorithms is linear in the number of tetrahedra and does neither depend on the number of isosurfaces nor on the employed transfer functions.", 
    "authors": "Rottger, S.;Kraus, M.;Ertl, T.", 
    "title": "Hardware-accelerated volume and isosurface rendering based on cell-projection"
  }, 
  {
    "abstract": "Large area tiled displays are gaining popularity for use in collaborative immersive virtual environments and scientific visualization. While recent work has addressed the issues of geometric registration, rendering architectures, and human interfaces, there has been relatively little work on photometric calibration in general, and photometric non-uniformity in particular. For example, as a result of differences in the photometric characteristics of projectors, the color and intensity of a large area display varies from place to place. Further, the imagery typically appears brighter at the regions of overlap between adjacent projectors. We analyze and classify the causes of photometric non-uniformity in a tiled display. We then propose a methodology for determining corrections designed to achieve uniformity, that can correct for the photometric variations across a tiled projector display in real time using per channel color look-up-tables (LUT).", 
    "authors": "Majumder, A.;Zhu He;Towles, H.;Welch, G.", 
    "title": "Achieving color uniformity across multi-projector displays"
  }, 
  {
    "abstract": "A scalable, high-resolution display may be constructed by tiling many projected images over a single display surface. One fundamental challenge for such a display is to avoid visible seams due to misalignment among the projectors. Traditional methods for avoiding seams involve sophisticated mechanical devices and expensive CRT projectors, coupled with extensive human effort for fine-tuning the projectors. The paper describes an automatic alignment method that relies on an inexpensive, uncalibrated camera to measure the relative mismatches between neighboring projectors, and then correct the projected imagery to avoid seams without significant human effort.", 
    "authors": "Chen, Y.;Clark, D.W.;Finkelstein, A.;Housel, T.C.;Li, K.", 
    "title": "Automatic alignment of high-resolution multi-projector displays using an uncalibrated camera"
  }, 
  {
    "abstract": "Specific rendering modes are developed for a combined visual/haptic interface to allow exploration and understanding of fluid dynamics data. The focus is on visualization of shock surfaces and vortex cores. Advantages provided by augmenting traditional graphical rendering modes with haptic rendering modes are discussed. Particular emphasis is placed on synergistic combinations of visual and haptic modes which enable rapid, exploratory interaction with the data. Implementation issues are also discussed.", 
    "authors": "Lawrence, D.A.;Lee, C.D.;Pao, Lucy Y.;Novoselov, R.Y.", 
    "title": "Shock and vortex visualization using a combined visual/haptic interface"
  }, 
  {
    "abstract": "We present an algorithm for haptic display of moderately complex polygonal models with a six degree of freedom (DOF) force feedback device. We make use of incremental algorithms for contact determination between convex primitives. The resulting contact information is used for calculating the restoring forces and torques and thereby used to generate a sense of virtual touch. To speed up the computation, our approach exploits a combination of geometric locality, temporal coherence, and predictive methods to compute object-object contacts at kHz rates. The algorithm has been implemented and interfaced with a 6-DOF PHANToM Premium 1.5. We demonstrate its performance on force display of the mechanical interaction between moderately complex geometric structures that can be decomposed into convex primitives.", 
    "authors": "Gregory, A.;Mascarenhas, A.;Ehmann, S.;Ming Lin;Manocha, D.", 
    "title": "Six degree-of-freedom haptic display of polygonal models"
  }, 
  {
    "abstract": "We propose a technique for visualizing steady flow. Using this technique, we first convert the vector field data into a scalar level-set representation. We then analyze the dynamic behavior and subsequent distortion of level-sets and interactively monitor the evolving structures by means of texture-based surface rendering. Next, we combine geometrical and topological considerations to derive a multiscale representation and to implement a method for the automatic placement of a sparse set of graphical primitives depicting homogeneous streams in the fields. Using the resulting algorithms, we have built a visualization system that enables us to effectively display the flow direction and its dynamics even for dense 3D fields.", 
    "authors": "Westermann, R.;Johnson, C.;Ertl, T.", 
    "title": "A level-set method for flow visualization"
  }, 
  {
    "abstract": "We present a novel hardware-accelerated texture advection algorithm to visualize the motion of two-dimensional unsteady flows. Making use of several proposed extensions to the OpenGL-1.2 specification, we demonstrate animations of over 65,000 particles at 2 frames/sec on an SGI Octane with EMXI graphics. High image quality is achieved by careful attention to edge effects, noise frequency, and image enhancement. We provide a detailed description of the hardware implementation, including temporal and spatial coherence techniques, dye advection techniques, and feature extraction.", 
    "authors": "Jobard, B.;Erlebacher, G.;Hussaini, M.Y.", 
    "title": "Hardware-accelerated texture advection for unsteady flow visualization"
  }, 
  {
    "abstract": "The paper presents a seed placement strategy for streamlines based on flow features in the dataset. The primary goal of our seeding strategy is to capture flow patterns in the vicinity of critical points in the flow field, even as the density of streamlines is reduced. Secondary goals are to place streamlines such that there is sufficient coverage in non-critical regions, and to vary the streamline placements and lengths so that the overall presentation is aesthetically pleasing (avoid clustering of streamlines, avoid sharp discontinuities across several streamlines, etc.). The procedure is straightforward and non-iterative. First, critical points are identified. Next, the flow field is segmented into regions, each containing a single critical point. The critical point in each region is then seeded with a template depending on the type of critical point. Finally, additional seed points are randomly distributed around the field using a Poisson disk distribution to minimize closely spaced seed points. The main advantage of this approach is that it does not miss the features around critical points. Since the strategy is not image-guided, and hence not view dependent, significant savings are possible when examining flow fields from different viewpoints, especially for 3D flow fields.", 
    "authors": "Verma, V.;Kao, D.;Pang, A.", 
    "title": "A flow-guided streamline seeding strategy"
  }, 
  {
    "abstract": "The work presents a method to enable matching of level-of-detail (LOD) models to image-plane resolution over large variations in viewing distances often present in exterior images. A relationship is developed between image sampling rate, viewing distance, object projection, and expected image error due to LOD approximations. This is employed in an error metric to compute error profiles for LOD models. Multirate filtering in the frequency space of a reference object image is utilized to approximate multiple distant views over a range of orientations. An importance sampling method is described to better characterize perspective projection over view distance. A contrast sensitivity function (CSF) is employed to approximate the response of the vision system. Examples are presented for multiresolution spheres and a terrain height field feature. Future directions for extending this method are described.", 
    "authors": "Scoggins, R.K.;Machiraju, R.;Moorhead, R.J.", 
    "title": "Enabling level-of-detail matching for exterior scene synthesis"
  }, 
  {
    "abstract": "Distance judgments are difficult in current virtual environments,limiting their effectiveness in conveying spatial information. This problem is apparent when contact occurs while a user is manipulating objects. In particular, the computer graphics used to support current generation immersive interfaces does a poor job of providing the visual cues necessary to perceive when contact between objects is about to occur. This perception of imminent contact is important in human motor control. Its absence prevents a sense of naturalness in interactive displays which allow for object manipulation. This paper reports results from an experiment evaluating the effectiveness of binocular disparity, cast shadows, and diffuse interreflections in signaling imminent contact in a manipulation task.", 
    "authors": "Hu, H.H;Gooch, A.A.;Thompson, W.B.;Smits, B.E.;Rieser, J.J.;Shirley, P.", 
    "title": "Visual cues for imminent object contact in realistic virtual environment"
  }, 
  {
    "abstract": "This is basic research for assigning color values to voxels of multichannel MRI volume data. The MRI volume data sets obtained under different scanning conditions are transformed into their components by independent component analysis (ICA), which enhances the physical characteristics of the tissue. The transfer functions for generating color values from independent components are obtained using a radial basis function network, a kind of neural net, by training the network with sample data chosen from the Visible Female data set. The resultant color volume data sets correspond well with the full-color cross-sections of the Visible Human data sets.", 
    "authors": "Muraki, S.;Nakai, T.;Kita, Y.", 
    "title": "Basic research for coloring multichannel MRI data"
  }, 
  {
    "abstract": "Accurately and automatically conveying the structure of a volume model is a problem that has not been fully solved by existing volume rendering approaches. Physics-based volume rendering approaches create images which may match the appearance of translucent materials in nature but may not embody important structural details. Transfer function approaches allow flexible design of the volume appearance but generally require substantial hand-tuning for each new data set in order to be effective. We introduce the volume illustration approach, combining the familiarity of a physics-based illumination model with the ability to enhance important features using non-photorealistic rendering techniques. Since the features to be enhanced are defined on the basis of local volume characteristics rather than volume sample values, the application of volume illustration techniques requires less manual tuning than the design of a good transfer function. Volume illustration provides a flexible unified framework for enhancing structural perception of volume models through the amplification of features and the addition of illumination effects.", 
    "authors": "Ebert, D.;Rheingans, P.", 
    "title": "Volume illustration: non-photorealistic rendering of volume models"
  }, 
  {
    "abstract": "Concerns the development of non-photorealistic rendering techniques for volume visualisation. In particular, we present two pen-and-ink rendering methods, a 3D method based on non-photorealistic solid textures, and a 2+D method that involves two rendering phases in the object space and the image space respectively. As both techniques utilize volume- and image-based data representations, they can be built upon a traditional volume rendering pipeline, and can be integrated with the photorealistic methods available in such a pipeline. We demonstrate that such an integration facilitates an effective mechanism for enhancing visualisation and its interpretation.", 
    "authors": "Treavett, S.M.F.;Chen, M.", 
    "title": "Pen-and-ink rendering in volume visualisation"
  }, 
  {
    "abstract": "Presents a two-level approach for fusing direct volume rendering (DVR) and maximum-intensity projection (MIP) within a joint rendering method. Different structures within the data set are rendered locally by either MIP or DVR on an object-by-object basis. Globally, all the results of subsequent object renderings are combined in a merging step (usually compositing in our case). This allows us to selectively choose the most suitable technique for depicting each object within the data, while keeping the amount of information contained in the image at a reasonable level. This is especially useful when inner structures should be visualized together with semi-transparent outer parts, similar to the focus-and-context approach known from information visualization. We also present an implementation of our approach which allows us to explore volumetric data using two-level rendering at interactive frame rates.", 
    "authors": "Hauser, H.;Mroz, L.;Bischi, G.-I.;Groller, M.E.", 
    "title": "Two-level volume rendering - fusing MIP and DVR"
  }, 
  {
    "abstract": "Splatting is widely applied in many areas, including volume, point-based and image-based rendering. Improvements to splatting, such as eliminating popping and color bleeding, occasion-based acceleration, post-rendering classification and shading, have all been recently accomplished. These improvements share a common need for efficient frame-buffer accesses. We present an optimized software splatting package, using a newly designed primitive, called FastSplat, to scan-convert footprints. Our approach does not use texture mapping hardware, but supports the whole pipeline in memory. In such an integrated pipeline, we are then able to study the optimization strategies and address image quality issues. While this research is meant for a study of the inherent trade-off of splatting, our renderer, purely in software, achieves 3- to 5-fold speedups over a top-end texture hardware implementation (for opaque data sets). We further propose a method of efficient occlusion culling using a summed area table of opacity. 3D solid texturing and bump mapping capabilities are demonstrated to show the flexibility of such an integrated rendering pipeline. A detailed numerical error analysis, in addition to the performance and storage issues, is also presented. Our approach requires low storage and uses simple operations. Thus, it is easily implementable in hardware.", 
    "authors": "Huang, Jian;Mueller, K.;Shareef, N.;Crawfis, R.", 
    "title": "FastSplats: optimized splatting on rectilinear grids"
  }, 
  {
    "abstract": "Presents a new rendering technique for processing multiple multi-resolution textures of LOD (level-of-detail) terrain models and describes its application to interactive, animated terrain content design. The approach is based on a multi-resolution model for terrain texture which cooperates with a multi-resolution model for terrain geometry. For each texture layer, an image pyramid and a texture tree are constructed. Multiple texture layers can be associated with one terrain model and can be combined in different ways, e.g. by blending and masking. The rendering algorithm simultaneously traverses the multi-resolution geometry model and the multi-resolution texture model, and takes into account geometric and texture approximation errors. It uses multi-pass rendering and exploits multi-texturing to achieve real-time performance. Applications include interactive texture lenses, texture animation and topographic textures. These techniques offer an enormous potential for developing new visualization applications for presenting, exploring and manipulating spatio-temporal data.", 
    "authors": "Dollner, J.;Baumann, K.;Hinrichs, K.", 
    "title": "Texturing techniques for terrain visualization"
  }, 
  {
    "abstract": "Geometric models are often annotated to provide additional information during visualization. Maps may be marked with rivers, roads or topographical information, and CAD data models may highlight the underlying mesh structure. While this additional information may be extremely useful, there is a rendering cost associated with it. Texture maps have often been used to convey this information at relatively low cost, but they suffer from blurring and pixelization at high magnification. We present a technique for simplifying surface annotations based on directed, asymmetric tolerance. By maintaining the annotations as geometry, as opposed to textures, we are able to simplify them while still maintaining the overall appearance of the model over a wide range of magnifications. Texture maps may still be used to provide low-resolution surface detail, such as color. We demonstrate a significant gain in rendering performance while retaining the original appearance of objects from many application domains.", 
    "authors": "Suits, F.;Klosowski, J.T.;Horn, W.P.;Lecina, G.", 
    "title": "Simplification of surface annotations"
  }, 
  {
    "abstract": "Discusses the concept of uniform frequency images, which exhibit uniform local frequency properties. Such images make optimal use of space when sampled close to their Nyquist limit. A warping function may be applied to an arbitrary image to redistribute its local frequency content, reducing its highest frequencies and increasing its lowest frequencies in order to approach this uniform frequency ideal. The warped image may then be downsampled according to its new, reduced Nyquist limit, thereby reducing its storage requirements. To reconstruct the original image, the inverse warp is applied. We present a general, top-down algorithm to automatically generate a piecewise-linear warping function with this frequency balancing property for a given input image. The image size is reduced by applying the warp and then downsampling. We store this warped, downsampled image plus a small number of polygons with texture coordinates to describe the inverse warp. The original image is later reconstructed by rendering the associated polygons with the warped image applied as a texture map, a process which is easily accelerated by current graphics hardware. As compared to previous image compression techniques, we generate a similar graceful space-quality tradeoff with the advantage of being able to \"uncompress\" images during rendering. We report results for several images with sizes ranging from 15,000 to 300,000 pixels, achieving reduction rates of 70-90% with improved quality over downsampling alone.", 
    "authors": "Hunter, A.;Cohen, J.D.", 
    "title": "Uniform frequency images: adding geometry to images to produce space-efficient textures"
  }, 
  {
    "abstract": "This paper presents an efficient keyframeless image-based rendering technique. An intermediate image is used to exploit the coherences among neighboring frames. The pixels in the intermediate image are first rendered by a ray-casting method and then warped to the intermediate image at the current viewpoint and view direction. We use an offset buffer to record the precise positions of these pixels in the intermediate image. Every frame is generated in three steps: warping the intermediate image onto the frame, filling in holes, and selectively rendering a group of \u201dold\u201d pixels. By dynamically adjusting the number of those \u201dold\u201d pixels in the last step, the workload at every frame can be balanced. The pixels generated by the last two steps make contributions to the new intermediate image. Unlike occasional keyframes in conventional image-based rendering which need to be totally rerendered, intermediate images only need to be partially updated at every frame. In this way, we guarantee more stable frame rates and more uniform image qualities. The intermediate image can be warped efficiently by a modified incremental 3D warp algorithm. As a specific application, we demonstrate our technique with a voxel-based terrain rendering system.\"", 
    "authors": "Qu, H.;Wan, M;Qin, J.;Kaufman, A.", 
    "title": "Image based rendering with stable frame rates"
  }, 
  {
    "abstract": "Multiresolution methods are becoming increasingly important tools for the interactive visualization of very large data sets. Multiresolution isosurface visualization allows the user to explore volume data using simplified and coarse representations of the isosurface for overview images, and finer resolution in areas of high interest or when zooming into the data. Ideally, a coarse isosurface should have the same topological structure as the original. The topological genus of the isosurface is one important property which is often neglected in multiresolution algorithms. This results in uncontrolled topological changes which can occur whenever the level-of-detail is changed. The scope of this paper is to propose an efficient technique which allows preservation of topology as well as controlled topology simplification in multiresolution isosurface extraction.", 
    "authors": "Gerstner, T.;Pajarola, R.", 
    "title": "Topology preserving and controlled topology simplifying multiresolution isosurface extraction"
  }, 
  {
    "abstract": "Visualization algorithms have seen substantial improvements in the past several years. However, very few algorithms have been developed for directly studying data in dimensions higher than three. Most algorithms require a sampling in three-dimensions before applying any visualization algorithms. This sampling typically ignores vital features that may be present when examined in oblique cross-sections, and places an undo burden on system resources when animation through additional dimensions is desired. For time-varying data of large data sets, smooth animation is desired at interactive rates. We provide a fast Marching Cubes like algorithm for hypercubes of any dimension. To support this, we have developed a new algorithm to automatically generate the isosurface and triangulation tables for any dimension. This allows the efficient calculation of 4D isosurfaces, which can be interactively sliced to provide smooth animation or slicing through oblique hyperplanes. The former allows for smooth animation in a very compressed format. The latter provide better tools to study time-evolving features as they move downstream. We also provide examples in using this technique to show interval volumes or the sensitivity of a particular isovalue threshold.", 
    "authors": "Bhaniramka, P.;Wenger, R.;Crawfis, R.", 
    "title": "Isosurfacing in higher dimensions"
  }, 
  {
    "abstract": "We present a novel method to extract iso-surfaces from distance volumes. It generates high quality semi-regular multiresolution meshes of arbitrary topology. Our technique proceeds in two stages. First, a very coarse mesh with guaranteed topology is extracted. Subsequently an iterative multi-scale force-based solver refines the initial mesh into a semi-regular mesh with geometrically adaptive sampling rate and good aspect ratio triangles. The coarse mesh extraction is performed using a new approach we call surface wavefront propagation. A set of discrete iso-distance ribbons are rapidly built and connected while respecting the topology of the iso-surface implied by the data. Subsequent multi-scale refinement is driven by a simple force-based solver designed to combine good iso-surface fit and high quality sampling through reparameterization. In contrast to the Marching Cubes technique our output meshes adapt gracefully to the iso-surface geometry, have a natural multiresolution structure and good aspect ratio triangles, as demonstrated with a number of examples.", 
    "authors": "Wood, Z.J.;Desbrun, M.;Schroder, P.;Breen, D.", 
    "title": "Semi-regular mesh extraction from volumes"
  }, 
  {
    "abstract": "A standard way to segment medical imaging datasets is by tracing contours around regions of interest in parallel planar slices. Unfortunately, the standard methods for reconstructing three dimensional surfaces from those planar contours tend to be either complicated or not very robust. Furthermore, they fail to consistently mesh abutting structures which share portions of contours. We present a novel, straight-forward algorithm for accurately and automatically reconstructing surfaces from planar contours. Our algorithm is based on scanline rendering and separating surface extraction. By rendering the contours as distinctly colored polygons and reading back each rendered slice into a segmented volume, we reduce the complex problem of building a surface from planar contours to the much simpler problem of extracting separating surfaces from a classified volume. Our scanline surfacing algorithm robustly handles complex surface topologies such as bifurcations, embedded features and abutting surfaces.", 
    "authors": "Weinstein, D.", 
    "title": "Scanline surfacing: building separating surfaces from planar contours"
  }, 
  {
    "abstract": "Throughout the design cycle, visualization, whether a sketch scribbled on the back of a spare piece of paper or a fully detailed drawing, has been the mainstay of design: we need to see the product. One of the most important stages of the design cycle is the initial, or concept, stage and it is here that design variants occur in large numbers to be vetted quickly. At this initial stage the human element, the designer is crucial to the success of the product. We describe an interactive environment for concept design which recognises the needs of the designer, not only to see the product and make rapid modifications, but also to monitor the progress of their design towards some preferred solution. This leads to the notion of a design parameter space, typically high-dimensional, which must also be visualized in addition to the product itself. Using a module developed for IRIS Explorer design steering is presented as a navigation of this space in order to search for optimal designs, either manually or by local optimisation.", 
    "authors": "Wright, H.;Brodlie, K.;David, T.", 
    "title": "Navigating high-dimensional spaces to support design steering"
  }, 
  {
    "abstract": "Multi-dimensional entities are modeled, displayed and understood with a new algorithm vectorizing data of any dimensionality. This algorithm is called SBP; it is a vectorized generalization of parallel coordinates. Classic geometries of any dimensionality can be demonstrated to facilitate perception and understanding of the shapes generated by this algorithm. SBP images of a 4D line, a circle and 3D and 4D spherical helices are shown. A strategy for synthesizing multi-dimensional models matching multi-dimensional data is presented. Current applications include data mining; modeling data-defined structures of scientific interest such as protein structure and Calabi-Yau figures as multi-dimensional geometric entities; generating vector-fused data signature fingerprints of classic frequency spectra that identify substances; and treating complex targets as multi-dimensional entities for automatic target recognition. SBP vector data signatures apply to all pattern recognition problems.", 
    "authors": "Johnson, R.R.", 
    "title": "Visualization of multi-dimensional data with vector-fusion"
  }, 
  {
    "abstract": "This paper describes a novel rendering technique for special relativistic visualization. It is an image-based method which allows to render high speed flights through real-world scenes filmed by a standard camera. The relativistic effects on image generation are determined by the relativistic aberration of light, the Doppler effect, and the searchlight effect. These account for changes of apparent geometry, color and brightness of the objects. It is shown how the relativistic effects can be taken into account by a modification of the plenoptic function. Therefore, all known image-based nonrelativistic rendering methods can easily be extended to incorporate relativistic rendering. Our implementation allows interactive viewing of relativistic panoramas and the production of movies which show super-fast travel. Examples in the form of snapshots and film sequences are included.", 
    "authors": "Weiskopf, D.;Kobras, D.;Ruder, H.", 
    "title": "Real-world relativity: image-based special relativistic visualization"
  }, 
  {
    "abstract": "One of the main research topics in scientific visualization is to \"visualize the appropriate features\" of a certain structure or data set. Geodesics are very important in geometry and physics, but there is one major problem which prevents scientists from using them as a visualization tool: the differential equations for geodesics are very complicated and in most cases numerical algorithms must be used. There is always a certain approximation error involved. How can you be sure to visualize the features and not only the approximation quality. The paper presents an algorithm to overcome this problem. It consists of two parts. In the first, a geometric method for the construction of geodesics of arbitrary surfaces is introduced. This method is based on the fundamental property that geodesics are a generalization of straight lines on plains. In the second part these geodesics are used to generate local nets on the surfaces.", 
    "authors": "Hotz, I.;Hagen, H.", 
    "title": "Visualizing geodesics"
  }, 
  {
    "abstract": "The compression of geometric structures is a relatively new field of data compression. Since about 1995, several articles have dealt with the coding of meshes, using for most of them the following approach: the vertices of the mesh are coded in an order that partially contains the topology of the mesh. In the same time, some simple rules attempt to predict the position of each vertex from the positions of its neighbors that have been previously coded. We describe a compression algorithm whose principle is completely different: the coding order of the vertices is used to compress their coordinates, and then the topology of the mesh is reconstructed from the vertices. This algorithm achieves compression ratios that are slightly better than those of the currently available algorithms, and moreover, it allows progressive and interactive transmission of the meshes.", 
    "authors": "Devillers, O.;Gandoin, P.-M.", 
    "title": "Geometric compression for interactive transmission"
  }, 
  {
    "abstract": "In 1998 we introduced the idea for a project we call the Office of the Future. Our long-term vision is to provide a better every-day working environment, with high-fidelity scene reconstruction for life-sized 3D tele-collaboration. In particular, we want a true sense of presence with our remote collaborator and their real surroundings. The challenges related to this vision are enormous and involve many technical tradeoffs. This is true in particular for scene reconstruction. Researchers have been striving to achieve real-time approaches, and while they have made respectable progress, the limitations of conventional technologies relegate them to relatively low resolution in a restricted volume. We present a significant step toward our ultimate goal, via a slightly different path. In lieu of low-fidelity dynamic scene modeling we present an exceedingly high fidelity reconstruction of a real but static office. By assembling the best of available hardware and software technologies in static scene acquisition, modeling algorithms, rendering, tracking and stereo projective display, we are able to demonstrate a portal to a real office, occupied today by a mannequin, and in the future by a real remote collaborator. We now have both a compelling sense of just how good it could be, and a framework into which we will later incorporate dynamic scene modeling, as we continue to head toward our ultimate goal of 3D collaborative telepresence.", 
    "authors": "Wei-Chao Wen;Towles, H.;Nyland, L.;Welch, G.;Fuchs, H.", 
    "title": "Toward a compelling sensation of telepresence: demonstrating a portal to a distant (static) office"
  }, 
  {
    "abstract": "In this paper we are presenting a novel architecture which allows rendering of large-shared dataset at interactive rates on an inexpensive workstation. The idea is based on view-dependent rendering on a client-server network. The server stores the large dataset and manages the selection of the various levels of detail while the inexpensive clients receive a stream of update operations that generate the appropriate level of detail in an incremental fashion. These update operations are based on changes in the clients\u2019 view-parameters. Our approach dramatically reduces the amount of memory needed by each client and the entire computing system since the dataset is stored only once on the server\u2019s local memory. In addition, it decreases the load on the network as results of the incremental update contributed by view-dependent rendering.", 
    "authors": "El-Sana, J.", 
    "title": "multi-user view-dependent rendering"
  }, 
  {
    "abstract": "We present an algorithm for compressing 2D vector fields that preserves topology. Our approach is to simplify the given data set using constrained clustering. We employ different types of global and local error metrics including the earth mover's distance metric to measure the degradation in topology as well as weighted magnitude and angular errors. As a result, we obtain precise error bounds in the compressed vector fields. Experiments with both analytic and simulated data sets are presented. Results indicate that one can obtain significant compression with low errors without losing topology information.", 
    "authors": "Lodha, S.K.;Renteria, J.C.;Roskin, K.M.", 
    "title": "Topology preserving compression of 2D vector fields"
  }, 
  {
    "abstract": "A new method for the simplification of flow fields is presented. It is based on continuous clustering. A well-known physical clustering model, the Cahn Hillard model which describes phase separation, is modified to reflect the properties of the data to be visualized. Clusters are defined implicitly as connected components of the positivity set of a density function. An evolution equation for this function is obtained as a suitable gradient flow of an underlying anisotropic energy functional. Here, time serves as the scale parameter. The evolution is characterized by a successive coarsening of patterns\u2014the actual clustering \u2014 and meanwhile the underlying simulation data specifies preferable pattern boundaries. Here we discuss the applicability of this new type of approach mainly for flow fields, where the cluster energy penalizes cross streamline boundaries, but the method also carries provisions in other fields as well. The clusters are visualized via iconic representations. A skeletonization algorithm is used to find suitable positions for the icons.", 
    "authors": "Garcke, H.;Preuer, T.;Rumpf, M.;Telea, A.;Weikard, U.;van Wijk, J.J.", 
    "title": "A continuous clustering method for vector fields"
  }, 
  {
    "abstract": "Topology analysis of plane, turbulent vector fields results in visual clutter caused by critical points indicating vortices of finer and finer scales. A simplification can be achieved by merging critical points within a prescribed radius into higher order critical points. After building clusters containing the singularities to merge, the method generates a piecewise linear representation of the vector field in each cluster containing only one (higher order) singularity. Any visualization method can be applied to the result after this process. Using different maximal distances for the critical points to be merged results in a hierarchy of simplified vector fields that can be used for analysis on different scales.", 
    "authors": "Tricoche, X.;Scheuermann, G.;Hagen, H.", 
    "title": "A topology simplification method for 2D vector fields"
  }, 
  {
    "abstract": "We present a new algorithm for material boundary interface reconstruction from data sets containing volume fractions. We transform the reconstruction problem to a problem that analyzes the dual data set, where each vertex in the dual mesh has an associated barycentric coordinate tuple that represents the fraction of each material present. After constructing the dual tetrahedral mesh from the original mesh, we construct material boundaries by mapping a tetrahedron into barycentric space and calculating the intersections with Voronoi cells in barycentric space. These intersections are mapped back to the original physical space and triangulated to form the boundary surface approximation. This algorithm can be applied to any grid structure and can treat any number of materials per element/vertex.", 
    "authors": "Bonnell, K.S.;Schikore, D.R.;Joy, K.I.;Duchaineau, M.;Hamann, B.", 
    "title": "Constructing material interfaces from data sets with volume-fraction information"
  }, 
  {
    "abstract": "We present a novel approach to surface reconstruction based on the Delaunay complex. First we give a simple and fast algorithm that picks locally a surface at each vertex. For that, we introduce the concept of \u256c\u2557-intervals. It turns out that for smooth regions of the surface this method works very well and at difficult parts of the surface yields an output well-suited for postprocessing. As a postprocessing step we propose a topological clean up and a new technique based on linear programming in order to establish a topologically correct surface. These techniques should be useful also for many other reconstruction schemes.", 
    "authors": "Adamy, U.;Giesen, J.;John, M.", 
    "title": "New techniques for topologically correct surface reconstruction"
  }, 
  {
    "abstract": "Polyhedral meshes are used for visualization, computer graphics or geometric modeling purposes and result from many applications like iso-surface extraction, surface reconstruction or CAD/CAM. The paper introduces a method for constructing smooth surfaces from a triangulated polyhedral mesh of arbitrary topology. It presents a new algorithm which generalizes and improves the triangle 4-split method (S. Hahmann and G.-P. Bonneau) in the crucial point of boundary curve network construction. This network is then filled in by a visual smooth surface from which an explicit closed form parametrization is given. Furthermore, the method becomes now completely local and can interpolate normal vector input at the mesh vertices.", 
    "authors": "Bonneau, G.-P.;Hahmann, S.", 
    "title": "Polyhedral modeling"
  }, 
  {
    "abstract": "We present a segmentation approach to scientific visualization that combines the definition of higher-level data, the efficient extraction of meaningful derived feature-like data from defined properties, and the effective visual representation of the extracted data. Our frame- work is aimed at multi-valued time-varying data sets, where, for example, grid vertices might have a multitude of associated scalar, vector and tensor quantities. This \u201csegmentation\u201d approach to massive data set exploration allows the user to focus upon regions, and interactively explore these regions efficiently. The challenge is to generate this segmented data from existing multi-valued data sets, store this data in an efficient scheme, generate the boundaries of each region, and display these boundaries to the user. We present an integrated scheme that allows a common representation for seg- mentation, allows it to be applied to a number of data types, and allows derived representations to be calculated. We illustrate this framework with examples from scalar-and vector-field visualization.\n", 
    "authors": "Bennett, J.;Mahrou, K.;Hamann, B.;Joy, K.I.", 
    "title": "Bicubic subdivision-surface wavelets for large-scale isosurface representation and visualization"
  }, 
  {
    "abstract": "A new multiscale method in surface processing is presented which combines the image processing methodology based on nonlinear diffusion equations and the theory of geometric evolution problems. Its aim is to smooth discretized surfaces while simultaneously enhancing geometric features such as edges and corners. This is obtained by an anisotropic curvature evolution, where time is the multiscale parameter. Here, the diffusion tensor depends on the shape operator of the evolving surface. A spatial finite element discretization on arbitrary unstructured triangular meshes and a semi-implicit finite difference discretization in time are the building blocks of the easy to code algorithm presented. The systems of linear equations in each timestep are solved by appropriate, preconditioned iterative solvers. Different applications underline the efficiency and flexibility of the presented type of surface processing tool.", 
    "authors": "Clarenz, U.;Diewald, U.;Rumpf, M.", 
    "title": "Anisotropic geometric diffusion in surface processing"
  }, 
  {
    "abstract": "The concept of fairing applied to irregular triangular meshes has become more and more important. Previous contributions constructed better fairing operators, and applied them both to multiresolution editing tools and to multiresolution representations of meshes. In this paper, we generalize these powerful techniques to handle non-manifold models. Our framework computes a multilevel fairing of models by fairing both the two-manifold surfaces that define the model, the so-called two features, and all the boundary and intersection curves of the model, the so-called one-features. In addition we introduce two extensions that can be used in our framework as well as in manifold fairing concepts: an exact local volume preservation strategy and a method for feature preservation. Our framework works with any of the manifold fairing operators for meshes.", 
    "authors": "Hubeli, A.;Gross, M.", 
    "title": "Fairing of non-manifolds for visualization"
  }, 
  {
    "abstract": "We present an algorithm for automatically classifying the interior and exterior parts of a polygonal model. The need for visualizing the interiors of objects frequently arises in medical visualization and CAD modeling. The goal of such visualizations is to display the model in a way that the human observer can easily understand the relationship between the different parts of the surface. While there exist excellent methods for visualizing surfaces that are inside one another (nested surfaces), the determination of which parts of the surface are interior is currently done manually. Our automatic method for interior classification takes a sampling approach using a collection of direction vectors. Polygons are said to be interior to the model if they are not visible in any of these viewing directions from a point outside the model. Once we have identified polygons as being inside or outside the model, these can be textured or have different opacities applied to them so that the whole model can be rendered in a more comprehensible manner. An additional consideration for some models is that they may have holes or tunnels running through them that are connected to the exterior surface. Although an external observer can see into these holes. It is often desirable to mark the walls of such tunnels as being part of the interior of a model. In order to allow this modified classification of the interior, we use morphological operators to close all the holes of the model. An input model is used together with its closed version to provide a better classification of the portions of the original model.", 
    "authors": "Nooruddin, F.S.;Turk, G.", 
    "title": "Interior/exterior classification of polygonal models"
  }, 
  {
    "abstract": "Multi-resolution techniques and models have been shown to be effective for the display and transmission of large static geometric object. Dynamic environments with internally deforming models and scientific simulations using dynamic meshes pose greater challenges in terms of time and space, and need the development of similar solutions. We introduce the T-DAG, an adaptive multi-resolution representation for dynamic meshes with arbitrary deformations including attribute, position, connectivity and topology changes. T-DAG stands for time-dependent directed acyclic graph which defines the structure supporting this representation. We also provide an incremental algorithm (in time) for constructing the T-DAG representation of a given input mesh. This enables the traversal and use of the multi-resolution dynamic model for partial playback while still constructing new time-steps.", 
    "authors": "Shamir, A.;Pascucci, V.;Bajaj, C.", 
    "title": "Multi-resolution dynamic meshes with arbitrary deformations"
  }, 
  {
    "abstract": "Visualization can be an important tool for displaying, categorizing and digesting large quantities of inter-related information during laboratory and simulation experiments. Summary visualizations that compare and represent data sets in the context of a collection are particularly valuable. Applicable visualizations used in these settings must be fast (near real time) and should allow the addition of data sets as they are acquired without requiring rerendering of the visualization. This paper examines several visualization techniques for representing collections of data sets in a combustion experiment including spectral displays, tiling and geometric mappings of symmetry. The application provides insight into how such visualizations might be used in practical real-time settings to assist in exploration and in conducting parameter space surveys.", 
    "authors": "Robbins, K.A.;Gorman, M.", 
    "title": "Fast visualization methods for comparing dynamics: a case study in combustion"
  }, 
  {
    "abstract": "The display of iso-surfaces in medical data sets is an important visualization technique used by radiologists for the diagnosis of volumetric density data sets. The demands put by radiologists on such a display technique are interactivity, multiple stacked transparent surfaces and cutting planes that allow an interactive clipping of the surfaces. This paper presents a Java based, platform independent implementation of a very fast surface rendering algorithm which combines the advantages of explicit surface representation, splatting, and shear-warp projection to fulfill all these requirements. The algorithm is implemented within the context of J-Vision, an application for viewing and diagnosing medical images which is currently in use at various hospitals.", 
    "authors": "Mroz, L.;Wegenkittl, R.;Groller, E.", 
    "title": "Mastering interactive surface rendering for Java-based diagnostic applications"
  }, 
  {
    "abstract": "The paper describes a computer modeling and simulation system that supports computational steering, which is an effort to make the typical simulation workflow more efficient. Our system provides an interface that allows scientists to perform all of the steps in the simulation process in parallel and online. It uses a standard network flow visualization package, which has been extended to display graphical output in an immersive virtual environment such as a CAVE. Our system allows scientists to interactively manipulate simulation parameters and observe the results. It also supports inverse steering, where the user specifies the desired simulation result, and the system searches for the simulation parameters that achieve this result. Taken together, these capabilities allow scientists to more efficiently and effectively understand model behavior, as well as to search through simulation parameter space. The paper is also a case study of applying our system to the problem of simulating microwave interactions with missile bodies. Because these interactions are difficult to study experimentally, and have important effects on missile electronics, there is a strong desire to develop and validate simulation models of this phenomena.", 
    "authors": "Swan, J.E.;Lanzagorta, M.;Maxwell, D.;Kuo, E.;Uhlmann, J.;Anderson, W.;Haw-Jye Shyu;Smith, W.", 
    "title": "A computational steering system for studying microwave interactions with missile bodies"
  }, 
  {
    "abstract": "General relativistic ray tracing is presented as a tool for gravitational physics. It is shown how standard three-dimensional ray tracing can be extended to allow for general relativistic visualization. This visualization technique provides images as seen by an observer under the influence of a gravitational field and allows to probe space-time by null geodesics. Moreover, a technique is proposed for visualizing the caustic surfaces generated by a gravitational lens. The suitability of general relativistic ray tracing is demonstrated by means of two examples, namely the visualization of the rigidly rotating disk of dust and the warp drive metric.", 
    "authors": "Weiskopf, D.", 
    "title": "Four-dimensional non-linear ray tracing as a visualization tool for gravitational physics"
  }, 
  {
    "abstract": "For a comprehensive understanding of tomographic image data in medicine, interactive and high-quality direct volume rendering is an essential prerequisite. This is provided by visualization using 3D texture mapping which is still limited to high-end graphics hardware. In order to make it available in a clinical environment, we present a system which uniquely combines local desktop computers and remote high-end graphics hardware. In this context, we exploit the standard visualization capabilities to a maximum which are available in the clinical environment. For 3D representations of high resolution and quality we access the remote specialized hardware. Various tools for 2D and 3D visualization are provided which meet the requirements of a medical diagnosis. This is demonstrated with examples from the field of neuroradiology which show the value of our strategy in practice.", 
    "authors": "Engel, K.;Hastreiter, P.;Tomandl, B.;Eberhardt, K.;Ertl, T.", 
    "title": "Combining local and remote visualization techniques for interactive volume rendering in medical applications"
  }, 
  {
    "abstract": "We describe a toolkit for the design and visualization of flexible artificial heart valves. The toolkit consists of interlinked modules with a visual programming interface. The user of the toolkit can set the initial geometry and material properties of the valve leaflet, solve for the flexing of the leaflet and the flow of blood around it, and display the results using the visualization capabilities of the toolkit. The interactive nature of our environment is highlighted by the fact that changes in leaflet properties are immediately reflected in the flow field and response of the leaflet. Hence the user may, in a single session, investigate a broad range of designs, each one of which provides important information about the blood flow and motion of the valve during the cardiac cycle.", 
    "authors": "Fenlon, A.J.;David, T.;Walton, J.P.R.B.", 
    "title": "An integrated visualization and design toolkit for flexible prosthetic heart valves"
  }, 
  {
    "abstract": "We present an immersive system for exploring numerically simulated flow data through a model of a coronary artery graft. This tightly-coupled interdisciplinary project is aimed at understanding how to reduce the failure rate of these grafts. The visualization system provides a mechanism for exploring the effect of changes to the geometry, to the flow, and for exploring potential sources of future lesions. The system uses gestural and voice interactions exclusively, moving away from more traditional windows/icons/menus/point-and-click (WIMP) interfaces. We present an example session using the system and discuss our experiences developing, testing, and using it. We describe some of the interaction and rendering techniques that we experimented with and describe their level of success. Our experience suggests that systems like this are exciting to clinical researchers, but conclusive evidence of their value is not yet available.", 
    "authors": "Forsberg, A.;Laidlaw, D.H.;van Dam, A.;Kirby, R.M.;Kafniadakis, G.E.;Elion, J.L.", 
    "title": "Immersive virtual reality for visualizing flow through an artery"
  }, 
  {
    "abstract": "Virtual endoscopy presents the cross-sectional acquired 3D-data of a computer tomograph as an endoluminal view. The common approach for the visualization of a virtual endoscopy is surface rendering, yielding images close to a real endoscopy. If external structures are of interest, volume rendering techniques have to be used. These methods do not display the exact shape of the inner lumen very well. For certain applications, e.g. operation planning of a transbronchial biopsy, both the shape of the inner lumen as well as outer structures like blood vessels and the tumor have to be delineated. A method is described, that allows a quick and easy hybrid visualization using overlays of different visualization methods like different surfaces or volume renderings with different transfer functions in real time on a low-end PC. To achieve real time frame rates, image based rendering techniques have been used.", 
    "authors": "Wegenkittl, R.;Vilanova, A.;Hegedust, B.;Wagner, Daniel;Freund, M.C.;Groller, E.M.", 
    "title": "Mastering interactive virtual bronchioscopy on a low-end PC"
  }, 
  {
    "abstract": "The study of time dependent characteristics of proteins is important for gaining insight into many biological processes. However, visualizing protein dynamics by animating atom trajectories does not provide satisfactory results. When the trajectory is sampled with large times steps, the impression of smooth motion will be destroyed due to the effects of temporal aliasing. Sampling with small time steps will result in the camouflage of interesting motions. In this case study, we discuss techniques for the interactive 3D visualization of the dynamics of the photoactive yellow protein. We use essential dynamics methods to filter out uninteresting atom motions from the larger concerted motions. In this way, clear and concise 3D animations of protein motions can be produced. In addition, we discuss various interactive techniques that allow exploration of the essential subspace of the protein. We discuss the merits of these techniques when applied to the analysis of the yellow protein.", 
    "authors": "Huitema, H.;van Liere, R.", 
    "title": "Interactive visualization of protein dynamics"
  }, 
  {
    "abstract": "The authors present a visualization system for interactive real time animation and visualization of simulation results from a parallel Particle-in-Cell code. The system was designed and implemented for the Onyx2 Infinite Reality hardware. A number of different visual objects, such as volume rendered particle density functionals were implemented. To provide sufficient frame rates for interactive visualization, the system was designed to provide performance close to the hardware specifications both in terms of the I/O and graphics subsystems. The presented case study applies the developed system to the evolution of an instability that gives rise to a plasma surfatron, a mechanism which rapidly can accelerate particles to very high velocities and thus be of great importance in the context of electron acceleration in astrophysical shocks, in the solar corona and in particle accelerators. The produced visualizations have allowed us to identify a previously unknown saturation mechanism for the surfatron and direct research efforts into new areas of interest.", 
    "authors": "Ljung, P.;Dieckmann, M.;Andersson, N.;Ynnerman, A.", 
    "title": "Interactive visualization of particle-in-cell simulations"
  }, 
  {
    "abstract": "The microscopic analysis of time dependent 3D live cells provides considerable challenges to visualization. Effective visualization can provide insight into the structure and functioning of living cells. The paper presents a case study in which a number of visualization techniques were applied to analyze a specific problem in cell biology: the condensation and de-condensation of chromosomes during cell division. The spatial complexity of the data required sophisticated presentation techniques. The interactive virtual reality enabled visualization system, proteus, specially equipped for time dependent 3D data sets is described. An important feature of proteus is that it is extendible to cope with application-specific demands.", 
    "authors": "De Leeuw, W.C.;van Liere, R.;Verschure, P.J.;Visser, A.E.;Manders, E.M.M.;van Driel, R.", 
    "title": "Visualization of time dependent confocal microscopy data"
  }, 
  {
    "abstract": "Non-traditional applications of scientific data challenge the typical approaches to visualization. In particular popular scientific visualization strategies fail when the expertise of the data consumer is in a different field than the one that generated the data and data from the user's domain must be utilized as well. This problem occurs when predictive weather simulations are used for a number of weather-sensitive applications. A data fusion approach is adopted for visualization design and utilized for specific example problems.", 
    "authors": "Treinish, L.A.", 
    "title": "Visual data fusion for applications of high-resolution numerical weather prediction"
  }, 
  {
    "abstract": "Applications of visualization techniques that facilitate comparison of simulation and field datasets of seafloor hydrothermal plumes are demonstrated in order to explore and confirm theories of plume behavior. In comparing these datasets, there is no one-to-one correspondence. We show the comparison by performing quantitative capturing of large scale observable features. The comparisons are needed not only to improve the relevance of the simulations to the field observations, but also to enable real time adjustment of shipboard data collection systems. Our approach for comparing simulation and field datasets is to use skeletonization and centerline representation. Features representing plumes are skeletonized. Skeleton points are used to construct a centerline and to quantify plume properties on planes normal to the centerline. These skeleton points are further used to construct an idealized cone representing a plume isosurface. The difference between the plume feature and the cone is identified as protrusions of turbulent eddies. Comparison of the simulation and field data sets through these abstractions illustrates how these abstractions characterize a plume.", 
    "authors": "Bemis, K.G.;Silver, D.;Rona, P.;Chengwei Feng", 
    "title": "Case study: a methodology for plume visualization with application to real-time acquisition and navigation"
  }, 
  {
    "abstract": "In our study of regional climate modeling and simulation, we frequently encounter vector fields that are crowded with large numbers of critical points. A critical point in a flow is where the vector field vanishes. While these critical points accurately reflect the topology of the vector fields, in our study only a subset of them is worth further investigation. We present a filtering technique based on the vorticity of the vector fields to eliminate the less interesting and sometimes sporadic critical points in a multiresolution fashion. The neighboring regions of the preserved features, which are characterized by strong shear and circulation, are potential locations of weather instability. We apply our feature filtering technique to a regional climate modeling data set covering East Asia in the summer of 1991.", 
    "authors": "Pak Chung Wong;Foote, H.;Leung, R.;Jurrus, E.;Adams, D.;Thomas, J.", 
    "title": "Vector fields simplification-a case study of visualizing climate modeling and simulation data sets"
  }, 
  {
    "abstract": "WEAVE (Workbench Environment for Analysis and Visual Exploration) is an environment for creating interactive visualization applications. WEAVE differs from previous systems in that it provides transparent linking between custom 3D visualizations and multidimensional statistical representations, and provides interactive color brushing between all visualizations. The authors demonstrate how WEAVE can be used to rapidly prototype a biomedical application, weaving together simulation data, measurement data, and 3D anatomical data concerning the propagation of excitation in the heart. These linked statistical and custom three-dimensional visualizations of the heart can allow scientists to more effectively study the correspondence of structure and behavior.", 
    "authors": "Gresh, D.L.;Rogowitz, B.E.;Winslow, R.L.;Scollan, D.F.;Yung, C.K.", 
    "title": "WEAVE: a system for visually linking 3-D and statistical visualizations applied to cardiac simulation and measurement data"
  }, 
  {
    "abstract": "Using inductive learning techniques to construct classification models from large, high-dimensional data sets is a useful way to make predictions in complex domains. However, these models can be difficult for users to understand. We have developed a set of visualization methods that help users to understand and analyze the behavior of learned models, including techniques for high-dimensional data space projection, display of probabilistic predictions, variable/class correlation, and instance mapping. We show the results of applying these techniques to models constructed from a benchmark data set of census data, and draw conclusions about the utility of these methods for model understanding.", 
    "authors": "Rheingans, P.;desJardins, M.", 
    "title": "Visualizing high-dimensional predictive model quality"
  }, 
  {
    "abstract": "Visualization techniques enable scientists to interactively explore 3D data sets, segmenting and cutting them to reveal inner structure. While powerful, these techniques suffer from one serious flaw-the images they create are displayed on a flat piece of glass or paper. It is not really 3D-it can only be made to appear 3D. We describe the construction of 3D physical models from volumetric data. Using solid freeform fabrication equipment, these models are built as separate interlocking pieces that express in physical form the segmentation and cutting operations common in display-based visualization.", 
    "authors": "Nadeau, David R.;Bailey, M.J.", 
    "title": "Visualizing volume data using physical models"
  }, 
  {
    "abstract": "We demonstrate the use of a combination of perceptually effective techniques for visualizing magnetic field data from the DIII-D Tokamak. These techniques can be implemented to run very efficiently on machines with hardware support for OpenGL. Interactive speeds facilitate clear communication of magnetic field structure, enhancing fusion scientists' understanding of their data, and thereby accelerating their research.", 
    "authors": "Schussman, G.;Kwan-Liu Ma;Schissel, D.;Evans, T.", 
    "title": "Visualizing DIII-D Tokamak magnetic field lines"
  }, 
  {
    "abstract": "The paper describes the effective real-time visualization of the clear-up operation of a former US nuclear submarine base, located in Holy Loch, Scotland. The Whole Field Modelling System has provided an extremely accurate real-time visualization of a large number of varying parameters such as remotely operated vehicles, cranes, barges, grabs, magnets, and detailed seabed topography. The system has improved the field staffs' spatial and temporal awareness of the underwater environment and facilitated decision-making within the complex offshore working environment.", 
    "authors": "Chapman, P.;Wills, D.;Stevens, P.;Brookes, G.", 
    "title": "Real-time visualization of the clear-up of a former US naval base"
  }, 
  {
    "abstract": "This paper describes our experience in designing and building a tool for visualizing the results of the CE-QUAL-ICM Three-Dimensional Eutrophication Model, as applied to water quality in the Chesapeake Bay. This model outputs a highly multidimensional dataset over very many timesteps \u2013 outstripping the capabilities of the visualization tools available to the research team. As part of the Army Engineer Research and Development Center (ERDC) Programming Environment and Training (PET) project, a special visualization tool was developed. This paper includes discussions on how the simulation data are handled efficiently, as well as how the issues of usability, flexibility and collaboration are addressed.", 
    "authors": "Stein, A.;Shih, A. M.;Baker, M. P.;Cerco, C. F.;Noel, M. R.", 
    "title": "Scientific visualization of water quality in the Chesapeake bay"
  }, 
  {
    "abstract": "Scaling of simulations challenges the effectiveness of conventional visualization methods. This problem becomes two-fold for mesoscale weather models that operate in near-real-time at cloud-scale resolution. For example, typical approaches to vector field visualization (e.g., wind) are based upon global methods, which may not illustrate detailed structure. In addition, such computations employ multi-resolution meshes to capture small-scale phenomena, which are not properly reflected in both vector and scalar realizations. To address the former critical point analysis and simple bandpass filtering of wind fields is employed for better seed point identification of streamline calculations. For the latter, an encapsulation of nested computational meshes is developed for general realization. It is then combined with the seed point calculation for an improved vector visualization of multi-resolution weather forecasting data.", 
    "authors": "Treinish, L.A.", 
    "title": "Multi-resolution visualization techniques for nested weather models"
  }
]